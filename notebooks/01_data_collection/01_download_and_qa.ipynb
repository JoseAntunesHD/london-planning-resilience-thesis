{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Barking & Dagenham ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:02,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 3 records\n",
      "[QA] Barking & Dagenham 2010 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Barking_and_Dagenham_2010.csv\n",
      "2011: 14 records\n",
      "[QA] Barking & Dagenham 2011 FLAGS=LOW_VOLUME_LT50 rows=14 file=planning_core_Barking_and_Dagenham_2011.csv\n",
      "2012: 13 records\n",
      "[QA] Barking & Dagenham 2012 FLAGS=LOW_VOLUME_LT50 rows=13 file=planning_core_Barking_and_Dagenham_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 17 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Barking & Dagenham 2013 FLAGS=LOW_VOLUME_LT50 rows=17 file=planning_core_Barking_and_Dagenham_2013.csv\n",
      "2014: 23 records\n",
      "[QA] Barking & Dagenham 2014 FLAGS=LOW_VOLUME_LT50 rows=23 file=planning_core_Barking_and_Dagenham_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 26 records\n",
      "[QA] Barking & Dagenham 2015 FLAGS=LOW_VOLUME_LT50 rows=26 file=planning_core_Barking_and_Dagenham_2015.csv\n",
      "2016: 28 records\n",
      "[QA] Barking & Dagenham 2016 FLAGS=LOW_VOLUME_LT50 rows=28 file=planning_core_Barking_and_Dagenham_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:00<00:00,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 19 records\n",
      "[QA] Barking & Dagenham 2017 FLAGS=LOW_VOLUME_LT50 rows=19 file=planning_core_Barking_and_Dagenham_2017.csv\n",
      "2018: 22 records\n",
      "[QA] Barking & Dagenham 2018 FLAGS=LOW_VOLUME_LT50 rows=22 file=planning_core_Barking_and_Dagenham_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 685 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:01<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 983 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:02<00:01,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 2288 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:02<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 1598 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:03<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 1406 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:03<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1277 records\n",
      "2025: 990 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:03<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Barnet ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 236 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 283 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 294 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 464 records\n",
      "2014: 601 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 888 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:01<00:01,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 1078 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:01<00:02,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 1272 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:02,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 1542 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:03<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 7258 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:05<00:05,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 7108 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:07<00:05,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 7507 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:09<00:04,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 6585 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:10<00:02,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 6112 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:12<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 6183 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:13<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 5858 records\n",
      "\n",
      "=== Bexley ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 39 records\n",
      "[QA] Bexley 2010 FLAGS=LOW_VOLUME_LT50 rows=39 file=planning_core_Bexley_2010.csv\n",
      "2011: 26 records\n",
      "[QA] Bexley 2011 FLAGS=LOW_VOLUME_LT50 rows=26 file=planning_core_Bexley_2011.csv\n",
      "2012: 21 records\n",
      "[QA] Bexley 2012 FLAGS=LOW_VOLUME_LT50 rows=21 file=planning_core_Bexley_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 27 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Bexley 2013 FLAGS=LOW_VOLUME_LT50 rows=27 file=planning_core_Bexley_2013.csv\n",
      "2014: 31 records\n",
      "[QA] Bexley 2014 FLAGS=LOW_VOLUME_LT50 rows=31 file=planning_core_Bexley_2014.csv\n",
      "2015: 35 records\n",
      "[QA] Bexley 2015 FLAGS=LOW_VOLUME_LT50 rows=35 file=planning_core_Bexley_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:00, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 52 records\n",
      "2017: 57 records\n",
      "2018: 95 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:00<00:00, 10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2738 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3032 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3393 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:01,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2960 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2458 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2220 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2411 records\n",
      "\n",
      "=== Brent ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 1 records\n",
      "[QA] Brent 2010 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 1 records\n",
      "[QA] Brent 2011 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2011.csv\n",
      "2012: 1 records\n",
      "[QA] Brent 2012 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 1 records\n",
      "[QA] Brent 2013 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 0 records\n",
      "[QA] Brent 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Brent_2014.csv\n",
      "2015: 1 records\n",
      "[QA] Brent 2015 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2015.csv\n",
      "2016: 0 records\n",
      "[QA] Brent 2016 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Brent_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 1 records\n",
      "[QA] Brent 2017 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Brent_2017.csv\n",
      "2018: 4653 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:03<00:02,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4214 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:04<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3867 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4484 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:02,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3826 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:01,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3356 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3140 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3048 records\n",
      "\n",
      "=== Bromley ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 80 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 95 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 115 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 101 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 139 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 211 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:01,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 233 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:01<00:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 310 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:01,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 413 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:02<00:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 5398 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:04<00:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 5524 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:04,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 6307 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:07<00:03,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 5362 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:08<00:02,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 4786 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:09<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 4624 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:10<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2065 records\n",
      "\n",
      "=== Camden ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:02,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 180 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:02,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 150 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 201 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 353 records\n",
      "2014: 388 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 387 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:01<00:01,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 343 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:01<00:01,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 253 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:01,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 247 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 1624 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 1596 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3001 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:03<00:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2161 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2882 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2137 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:05<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1648 records\n",
      "\n",
      "=== City of London ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 39 records\n",
      "[QA] City of London 2010 FLAGS=LOW_VOLUME_LT50 rows=39 file=planning_core_City_of_London_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 34 records\n",
      "[QA] City of London 2011 FLAGS=LOW_VOLUME_LT50 rows=34 file=planning_core_City_of_London_2011.csv\n",
      "2012: 72 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 63 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 88 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 89 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:01,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 108 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 133 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:01,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 232 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 1123 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:01<00:01,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 842 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:02<00:01,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 961 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:02<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 1069 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:02<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 1235 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:03<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1261 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1518 records\n",
      "\n",
      "=== Croydon ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Croydon 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Croydon_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Croydon 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Croydon_2011.csv\n",
      "2012: 2 records\n",
      "[QA] Croydon 2012 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Croydon_2012.csv\n",
      "2013: 5 records\n",
      "[QA] Croydon 2013 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Croydon_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 1 records\n",
      "[QA] Croydon 2014 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Croydon_2014.csv\n",
      "2015: 2 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:01,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Croydon 2015 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Croydon_2015.csv\n",
      "2016: 7 records\n",
      "[QA] Croydon 2016 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Croydon_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 3 records\n",
      "[QA] Croydon 2017 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Croydon_2017.csv\n",
      "2018: 26 records\n",
      "[QA] Croydon 2018 FLAGS=LOW_VOLUME_LT50 rows=26 file=planning_core_Croydon_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:02<00:03,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4962 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 5239 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:03,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5533 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 4694 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 4217 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3831 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3541 records\n",
      "\n",
      "=== Ealing ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 199 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:02,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 362 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:01<00:07,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 4180 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:02<00:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 5087 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:04<00:11,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 5845 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:05<00:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 6069 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:07<00:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 6345 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:08<00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 5694 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:09<00:09,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 5133 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:11<00:08,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 5103 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:12<00:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4989 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:14<00:05,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5642 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:15<00:04,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 5037 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:17<00:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 4901 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:18<00:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 4071 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:18<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2588 records\n",
      "\n",
      "=== Enfield ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 2 records\n",
      "[QA] Enfield 2010 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Enfield_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 1 records\n",
      "[QA] Enfield 2011 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Enfield_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Enfield 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Enfield_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Enfield 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Enfield_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 18 records\n",
      "[QA] Enfield 2014 FLAGS=LOW_VOLUME_LT50 rows=18 file=planning_core_Enfield_2014.csv\n",
      "2015: 26 records\n",
      "[QA] Enfield 2015 FLAGS=LOW_VOLUME_LT50 rows=26 file=planning_core_Enfield_2015.csv\n",
      "2016: 41 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Enfield 2016 FLAGS=LOW_VOLUME_LT50 rows=41 file=planning_core_Enfield_2016.csv\n",
      "2017: 46 records\n",
      "[QA] Enfield 2017 FLAGS=LOW_VOLUME_LT50 rows=46 file=planning_core_Enfield_2017.csv\n",
      "2018: 62 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3937 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3767 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:02,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4450 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:05<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3776 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:06<00:01,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3569 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:07<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3597 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:08<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3816 records\n",
      "\n",
      "=== Greenwich ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Greenwich 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Greenwich_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Greenwich 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Greenwich_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Greenwich 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Greenwich_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Greenwich 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Greenwich_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 2 records\n",
      "[QA] Greenwich 2014 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Greenwich_2014.csv\n",
      "2015: 1 records\n",
      "[QA] Greenwich 2015 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Greenwich_2015.csv\n",
      "2016: 4 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Greenwich 2016 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Greenwich_2016.csv\n",
      "2017: 3 records\n",
      "[QA] Greenwich 2017 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Greenwich_2017.csv\n",
      "2018: 5 records\n",
      "[QA] Greenwich 2018 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Greenwich_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3110 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 2866 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3286 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3157 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:05<00:01,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2896 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:06<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3020 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:07<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3025 records\n",
      "\n",
      "=== Hackney ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 2 records\n",
      "[QA] Hackney 2010 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Hackney_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 5 records\n",
      "[QA] Hackney 2011 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Hackney_2011.csv\n",
      "2012: 3 records\n",
      "[QA] Hackney 2012 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Hackney_2012.csv\n",
      "2013: 4 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Hackney 2013 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Hackney_2013.csv\n",
      "2014: 4 records\n",
      "[QA] Hackney 2014 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Hackney_2014.csv\n",
      "2015: 3 records\n",
      "[QA] Hackney 2015 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Hackney_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 3 records\n",
      "[QA] Hackney 2016 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Hackney_2016.csv\n",
      "2017: 6 records\n",
      "[QA] Hackney 2017 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Hackney_2017.csv\n",
      "2018: 13 records\n",
      "[QA] Hackney 2018 FLAGS=LOW_VOLUME_LT50 rows=13 file=planning_core_Hackney_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2150 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 1530 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3376 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:03<00:01,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2128 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2743 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2734 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1516 records\n",
      "\n",
      "=== Hammersmith & Fulham ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Hammersmith & Fulham 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2010.csv\n",
      "2011: 0 records\n",
      "[QA] Hammersmith & Fulham 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 0 records\n",
      "[QA] Hammersmith & Fulham 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:00, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Hammersmith & Fulham 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2013.csv\n",
      "2014: 0 records\n",
      "[QA] Hammersmith & Fulham 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2014.csv\n",
      "2015: 0 records\n",
      "[QA] Hammersmith & Fulham 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 2 records\n",
      "[QA] Hammersmith & Fulham 2016 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Hammersmith_and_Fulham_2016.csv\n",
      "2017: 0 records\n",
      "[QA] Hammersmith & Fulham 2017 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hammersmith_and_Fulham_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:00<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 4 records\n",
      "[QA] Hammersmith & Fulham 2018 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Hammersmith_and_Fulham_2018.csv\n",
      "2019: 3323 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3113 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3656 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:01,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3157 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:05<00:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2845 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:06<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2845 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3108 records\n",
      "\n",
      "=== Haringey ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Haringey 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Haringey 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Haringey 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Haringey 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 0 records\n",
      "[QA] Haringey 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2014.csv\n",
      "2015: 0 records\n",
      "[QA] Haringey 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Haringey_2015.csv\n",
      "2016: 1 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Haringey 2016 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Haringey_2016.csv\n",
      "2017: 2 records\n",
      "[QA] Haringey 2017 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Haringey_2017.csv\n",
      "2018: 4 records\n",
      "[QA] Haringey 2018 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Haringey_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3058 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3050 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:02,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3566 records\n",
      "2022: 3296 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:06<00:01,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2609 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:07<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2365 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:07<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2223 records\n",
      "\n",
      "=== Harrow ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 15 records\n",
      "[QA] Harrow 2010 FLAGS=LOW_VOLUME_LT50 rows=15 file=planning_core_Harrow_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 12 records\n",
      "[QA] Harrow 2011 FLAGS=LOW_VOLUME_LT50 rows=12 file=planning_core_Harrow_2011.csv\n",
      "2012: 6 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Harrow 2012 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Harrow_2012.csv\n",
      "2013: 7 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Harrow 2013 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Harrow_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 7 records\n",
      "[QA] Harrow 2014 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Harrow_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 20 records\n",
      "[QA] Harrow 2015 FLAGS=LOW_VOLUME_LT50 rows=20 file=planning_core_Harrow_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:00,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 11 records\n",
      "[QA] Harrow 2016 FLAGS=LOW_VOLUME_LT50 rows=11 file=planning_core_Harrow_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 34 records\n",
      "[QA] Harrow 2017 FLAGS=LOW_VOLUME_LT50 rows=34 file=planning_core_Harrow_2017.csv\n",
      "2018: 40 records\n",
      "[QA] Harrow 2018 FLAGS=LOW_VOLUME_LT50 rows=40 file=planning_core_Harrow_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 81 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:01<00:01,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 899 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:02<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4350 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:03<00:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3393 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2325 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1944 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:05<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1381 records\n",
      "\n",
      "=== Havering ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 3 records\n",
      "[QA] Havering 2010 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Havering_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 10 records\n",
      "[QA] Havering 2011 FLAGS=LOW_VOLUME_LT50 rows=10 file=planning_core_Havering_2011.csv\n",
      "2012: 4 records\n",
      "[QA] Havering 2012 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Havering_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Havering 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Havering_2013.csv\n",
      "2014: 6 records\n",
      "[QA] Havering 2014 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Havering_2014.csv\n",
      "2015: 5 records\n",
      "[QA] Havering 2015 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Havering_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 6 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Havering 2016 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Havering_2016.csv\n",
      "2017: 15 records\n",
      "[QA] Havering 2017 FLAGS=LOW_VOLUME_LT50 rows=15 file=planning_core_Havering_2017.csv\n",
      "2018: 21 records\n",
      "[QA] Havering 2018 FLAGS=LOW_VOLUME_LT50 rows=21 file=planning_core_Havering_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3182 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3421 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:03,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4197 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3419 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:01,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3163 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2834 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2885 records\n",
      "\n",
      "=== Hillingdon ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 1 records\n",
      "[QA] Hillingdon 2010 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Hillingdon_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 1 records\n",
      "[QA] Hillingdon 2011 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Hillingdon_2011.csv\n",
      "2012: 3 records\n",
      "[QA] Hillingdon 2012 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Hillingdon_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Hillingdon 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hillingdon_2013.csv\n",
      "2014: 0 records\n",
      "[QA] Hillingdon 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Hillingdon_2014.csv\n",
      "2015: 6 records\n",
      "[QA] Hillingdon 2015 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Hillingdon_2015.csv\n",
      "2016: 7 records\n",
      "[QA] Hillingdon 2016 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Hillingdon_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 8 records\n",
      "[QA] Hillingdon 2017 FLAGS=LOW_VOLUME_LT50 rows=8 file=planning_core_Hillingdon_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:01,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 33 records\n",
      "[QA] Hillingdon 2018 FLAGS=LOW_VOLUME_LT50 rows=33 file=planning_core_Hillingdon_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:02<00:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4077 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:04<00:03,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4378 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:03,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4668 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:02,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3970 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3742 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3385 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3170 records\n",
      "\n",
      "=== Hounslow ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 131 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 135 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 272 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:02,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 1293 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:01<00:03,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 2106 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:01<00:04,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 2267 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:02<00:04,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 2582 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:03<00:04,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 2369 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:03<00:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 2186 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:05<00:04,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4173 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:06<00:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4063 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:07<00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4605 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:08<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3837 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:09<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3653 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:10<00:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3270 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2753 records\n",
      "\n",
      "=== Islington ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:14,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 3867 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:01<00:13,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 3933 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:02<00:12,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 3868 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:03<00:11,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 4067 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:04<00:11,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 4529 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:06<00:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 4465 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:07<00:09,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 4372 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:08<00:08,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 4220 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:09<00:07,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 3686 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:09<00:05,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3319 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:10<00:04,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3200 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:11<00:03,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3369 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:12<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3747 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:13<00:01,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3178 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:14<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1950 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:14<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3103 records\n",
      "\n",
      "=== Kensington & Chelsea ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Kensington & Chelsea 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Kensington & Chelsea 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Kensington & Chelsea 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Kensington & Chelsea 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2013.csv\n",
      "2014: 0 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Kensington & Chelsea 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2014.csv\n",
      "2015: 0 records\n",
      "[QA] Kensington & Chelsea 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2015.csv\n",
      "2016: 0 records\n",
      "[QA] Kensington & Chelsea 2016 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2016.csv\n",
      "2017: 0 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Kensington & Chelsea 2017 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2017.csv\n",
      "2018: 0 records\n",
      "[QA] Kensington & Chelsea 2018 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kensington_and_Chelsea_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 5173 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:02,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4725 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:02,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5451 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:05<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 5017 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:01,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 5387 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 5708 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 4508 records\n",
      "\n",
      "=== Kingston ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Kingston 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Kingston 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Kingston 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2012.csv\n",
      "2013: 0 records\n",
      "[QA] Kingston 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2013.csv\n",
      "2014: 1 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Kingston 2014 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Kingston_2014.csv\n",
      "2015: 0 records\n",
      "[QA] Kingston 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2015.csv\n",
      "2016: 0 records\n",
      "[QA] Kingston 2016 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Kingston_2016.csv\n",
      "2017: 3 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Kingston 2017 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Kingston_2017.csv\n",
      "2018: 6 records\n",
      "[QA] Kingston 2018 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Kingston_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2765 records\n",
      "2020: 2903 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3621 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:03<00:01,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3518 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3034 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2765 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2686 records\n",
      "\n",
      "=== Lambeth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 49 records\n",
      "[QA] Lambeth 2010 FLAGS=LOW_VOLUME_LT50 rows=49 file=planning_core_Lambeth_2010.csv\n",
      "2011: 65 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 65 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 62 records\n",
      "2014: 78 records\n",
      "2015: 90 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 157 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 205 records\n",
      "2018: 209 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3930 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:02,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4027 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:02,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4382 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:05<00:02,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 4055 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:06<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3681 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:07<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3588 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:08<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3594 records\n",
      "\n",
      "=== Lewisham ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 1 records\n",
      "[QA] Lewisham 2010 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Lewisham_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Lewisham 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Lewisham_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Lewisham 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Lewisham_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Lewisham 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Lewisham_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 1 records\n",
      "[QA] Lewisham 2014 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Lewisham_2014.csv\n",
      "2015: 0 records\n",
      "[QA] Lewisham 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Lewisham_2015.csv\n",
      "2016: 2 records\n",
      "[QA] Lewisham 2016 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Lewisham_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 5 records\n",
      "[QA] Lewisham 2017 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Lewisham_2017.csv\n",
      "2018: 6 records\n",
      "[QA] Lewisham 2018 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Lewisham_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3387 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3063 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4214 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:02,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3515 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:05<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2951 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:06<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2609 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1743 records\n",
      "\n",
      "=== Merton ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 12 records\n",
      "[QA] Merton 2010 FLAGS=LOW_VOLUME_LT50 rows=12 file=planning_core_Merton_2010.csv\n",
      "2011: 13 records\n",
      "[QA] Merton 2011 FLAGS=LOW_VOLUME_LT50 rows=13 file=planning_core_Merton_2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 14 records\n",
      "[QA] Merton 2012 FLAGS=LOW_VOLUME_LT50 rows=14 file=planning_core_Merton_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 16 records\n",
      "[QA] Merton 2013 FLAGS=LOW_VOLUME_LT50 rows=16 file=planning_core_Merton_2013.csv\n",
      "2014: 24 records\n",
      "[QA] Merton 2014 FLAGS=LOW_VOLUME_LT50 rows=24 file=planning_core_Merton_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 32 records\n",
      "[QA] Merton 2015 FLAGS=LOW_VOLUME_LT50 rows=32 file=planning_core_Merton_2015.csv\n",
      "2016: 47 records\n",
      "[QA] Merton 2016 FLAGS=LOW_VOLUME_LT50 rows=47 file=planning_core_Merton_2016.csv\n",
      "2017: 69 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 81 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2352 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 2257 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 2634 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:03<00:01,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2177 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2012 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1932 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1806 records\n",
      "\n",
      "=== Newham ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Newham 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Newham_2010.csv\n",
      "2011: 0 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Newham 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Newham_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Newham 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Newham_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 12.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Newham 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Newham_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 2 records\n",
      "[QA] Newham 2014 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Newham_2014.csv\n",
      "2015: 3 records\n",
      "[QA] Newham 2015 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Newham_2015.csv\n",
      "2016: 1 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Newham 2016 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Newham_2016.csv\n",
      "2017: 2 records\n",
      "[QA] Newham 2017 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Newham_2017.csv\n",
      "2018: 6 records\n",
      "[QA] Newham 2018 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Newham_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3405 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 2736 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3095 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:01,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2898 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2646 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2532 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 2477 records\n",
      "\n",
      "=== Redbridge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 26 records\n",
      "[QA] Redbridge 2010 FLAGS=LOW_VOLUME_LT50 rows=26 file=planning_core_Redbridge_2010.csv\n",
      "2011: 32 records\n",
      "[QA] Redbridge 2011 FLAGS=LOW_VOLUME_LT50 rows=32 file=planning_core_Redbridge_2011.csv\n",
      "2012: 51 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 52 records\n",
      "2014: 86 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:01,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 132 records\n",
      "2016: 126 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:01,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 271 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:01,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 1223 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:02<00:02,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4681 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:03,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4412 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5103 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:03,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 4379 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:07<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3759 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:08<00:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2896 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3814 records\n",
      "\n",
      "=== Richmond ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 57 records\n",
      "2011: 46 records\n",
      "[QA] Richmond 2011 FLAGS=LOW_VOLUME_LT50 rows=46 file=planning_core_Richmond_2011.csv\n",
      "2012: 53 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 72 records\n",
      "2014: 140 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:01,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 101 records\n",
      "2016: 129 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 92 records\n",
      "2018: 100 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:02<00:02,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 4684 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:03,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4845 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:03,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5389 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 4674 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:08<00:02,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 4427 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:09<00:01,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 4303 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:10<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3906 records\n",
      "\n",
      "=== Southwark ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   6%|▋         | 1/16 [00:00<00:01,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 18 records\n",
      "[QA] Southwark 2010 FLAGS=LOW_VOLUME_LT50 rows=18 file=planning_core_Southwark_2010.csv\n",
      "2011: 7 records\n",
      "[QA] Southwark 2011 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Southwark_2011.csv\n",
      "2012: 2 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  19%|█▉        | 3/16 [00:00<00:01,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Southwark 2012 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Southwark_2012.csv\n",
      "2013: 11 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Southwark 2013 FLAGS=LOW_VOLUME_LT50 rows=11 file=planning_core_Southwark_2013.csv\n",
      "2014: 7 records\n",
      "[QA] Southwark 2014 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Southwark_2014.csv\n",
      "2015: 1 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Southwark 2015 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Southwark_2015.csv\n",
      "2016: 1 records\n",
      "[QA] Southwark 2016 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Southwark_2016.csv\n",
      "2017: 7 records\n",
      "[QA] Southwark 2017 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Southwark_2017.csv\n",
      "2018: 14 records\n",
      "[QA] Southwark 2018 FLAGS=LOW_VOLUME_LT50 rows=14 file=planning_core_Southwark_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 3762 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:03<00:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 4016 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:04<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 4544 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:06<00:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3822 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:08<00:02,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 3450 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:09<00:01,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 3451 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:10<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 3489 records\n",
      "\n",
      "=== Sutton ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 0 records\n",
      "[QA] Sutton 2010 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 0 records\n",
      "[QA] Sutton 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Sutton 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 0 records\n",
      "[QA] Sutton 2013 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2013.csv\n",
      "2014: 0 records\n",
      "[QA] Sutton 2014 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 0 records\n",
      "[QA] Sutton 2015 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 0 records\n",
      "[QA] Sutton 2016 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Sutton_2016.csv\n",
      "2017: 1 records\n",
      "[QA] Sutton 2017 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Sutton_2017.csv\n",
      "2018: 2 records\n",
      "[QA] Sutton 2018 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Sutton_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2441 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 2484 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:01,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 2807 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:01,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2568 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:04<00:01,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2296 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:05<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2047 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1853 records\n",
      "\n",
      "=== Tower Hamlets ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 1 records\n",
      "[QA] Tower Hamlets 2010 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Tower_Hamlets_2010.csv\n",
      "2011: 0 records\n",
      "[QA] Tower Hamlets 2011 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Tower_Hamlets_2011.csv\n",
      "2012: 0 records\n",
      "[QA] Tower Hamlets 2012 FLAGS=EMPTY_CHUNK|VALID_DATE_PARSE_FAIL_HIGH rows=0 file=planning_core_Tower_Hamlets_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 2 records\n",
      "[QA] Tower Hamlets 2013 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Tower_Hamlets_2013.csv\n",
      "2014: 4 records\n",
      "[QA] Tower Hamlets 2014 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Tower_Hamlets_2014.csv\n",
      "2015: 3 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Tower Hamlets 2015 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Tower_Hamlets_2015.csv\n",
      "2016: 560 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:01<00:02,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 3069 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:02<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 3032 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:03<00:03,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2706 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:04<00:03,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 2651 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:05<00:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 2685 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:05<00:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 2433 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:06<00:01,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2024 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:07<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 1865 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:07<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1906 records\n",
      "\n",
      "=== Waltham Forest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 2 records\n",
      "[QA] Waltham Forest 2010 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Waltham_Forest_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 7 records\n",
      "[QA] Waltham Forest 2011 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Waltham_Forest_2011.csv\n",
      "2012: 1 records\n",
      "[QA] Waltham Forest 2012 FLAGS=LOW_VOLUME_LT50 rows=1 file=planning_core_Waltham_Forest_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 6 records\n",
      "[QA] Waltham Forest 2013 FLAGS=LOW_VOLUME_LT50 rows=6 file=planning_core_Waltham_Forest_2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:01,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014: 53 records\n",
      "2015: 8 records\n",
      "[QA] Waltham Forest 2015 FLAGS=LOW_VOLUME_LT50 rows=8 file=planning_core_Waltham_Forest_2015.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 31 records\n",
      "[QA] Waltham Forest 2016 FLAGS=LOW_VOLUME_LT50 rows=31 file=planning_core_Waltham_Forest_2016.csv\n",
      "2017: 82 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:00<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018: 541 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:01,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 2317 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:02<00:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 3215 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 3829 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:02,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 3345 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:05<00:01,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 2846 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:06<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 2766 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:06<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 1760 records\n",
      "\n",
      "=== Wandsworth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 7 records\n",
      "[QA] Wandsworth 2010 FLAGS=LOW_VOLUME_LT50 rows=7 file=planning_core_Wandsworth_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 5 records\n",
      "[QA] Wandsworth 2011 FLAGS=LOW_VOLUME_LT50 rows=5 file=planning_core_Wandsworth_2011.csv\n",
      "2012: 10 records\n",
      "[QA] Wandsworth 2012 FLAGS=LOW_VOLUME_LT50 rows=10 file=planning_core_Wandsworth_2012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  31%|███▏      | 5/16 [00:00<00:01,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 3 records\n",
      "[QA] Wandsworth 2013 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Wandsworth_2013.csv\n",
      "2014: 8 records\n",
      "[QA] Wandsworth 2014 FLAGS=LOW_VOLUME_LT50 rows=8 file=planning_core_Wandsworth_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  44%|████▍     | 7/16 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 16 records\n",
      "[QA] Wandsworth 2015 FLAGS=LOW_VOLUME_LT50 rows=16 file=planning_core_Wandsworth_2015.csv\n",
      "2016: 17 records\n",
      "[QA] Wandsworth 2016 FLAGS=LOW_VOLUME_LT50 rows=17 file=planning_core_Wandsworth_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  56%|█████▋    | 9/16 [00:01<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 36 records\n",
      "[QA] Wandsworth 2017 FLAGS=LOW_VOLUME_LT50 rows=36 file=planning_core_Wandsworth_2017.csv\n",
      "2018: 49 records\n",
      "[QA] Wandsworth 2018 FLAGS=LOW_VOLUME_LT50 rows=49 file=planning_core_Wandsworth_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:01<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 75 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:01<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 990 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:03<00:02,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 5497 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:04<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 4769 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:05<00:01,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 4520 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:07<00:01,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 4325 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 4175 records\n",
      "\n",
      "=== Westminster ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010: 2 records\n",
      "[QA] Westminster 2010 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Westminster_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  12%|█▎        | 2/16 [00:00<00:01, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011: 4 records\n",
      "[QA] Westminster 2011 FLAGS=LOW_VOLUME_LT50 rows=4 file=planning_core_Westminster_2011.csv\n",
      "2012: 2 records\n",
      "[QA] Westminster 2012 FLAGS=LOW_VOLUME_LT50 rows=2 file=planning_core_Westminster_2012.csv\n",
      "2013: 3 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  25%|██▌       | 4/16 [00:00<00:01, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Westminster 2013 FLAGS=LOW_VOLUME_LT50 rows=3 file=planning_core_Westminster_2013.csv\n",
      "2014: 10 records\n",
      "[QA] Westminster 2014 FLAGS=LOW_VOLUME_LT50 rows=10 file=planning_core_Westminster_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  38%|███▊      | 6/16 [00:00<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: 12 records\n",
      "[QA] Westminster 2015 FLAGS=LOW_VOLUME_LT50 rows=12 file=planning_core_Westminster_2015.csv\n",
      "2016: 19 records\n",
      "[QA] Westminster 2016 FLAGS=LOW_VOLUME_LT50 rows=19 file=planning_core_Westminster_2016.csv\n",
      "2017: 37 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  50%|█████     | 8/16 [00:00<00:00, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Westminster 2017 FLAGS=LOW_VOLUME_LT50 rows=37 file=planning_core_Westminster_2017.csv\n",
      "2018: 80 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  62%|██████▎   | 10/16 [00:03<00:02,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: 9511 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  69%|██████▉   | 11/16 [00:05<00:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: 8055 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  75%|███████▌  | 12/16 [00:07<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: 8179 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  81%|████████▏ | 13/16 [00:09<00:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: 7772 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  88%|████████▊ | 14/16 [00:11<00:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: 7960 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:  94%|█████████▍| 15/16 [00:13<00:01,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: 8904 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years: 100%|██████████| 16/16 [00:15<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: 8489 records\n",
      "\n",
      "DONE.\n",
      "Parts written to: c:/Users/Jose Miguel/OneDrive/Ambiente de Trabalho/NOVA/TESE/planning_extract_core_parts\n",
      "QA log written to : c:/Users/Jose Miguel/OneDrive/Ambiente de Trabalho/NOVA/TESE/planning_extract_core_parts\\_qa_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# QA CONFIG\n",
    "# =========================================================\n",
    "STRICT = False  # True = raise errors; False = log warnings and continue\n",
    "\n",
    "KEY_COLS = [\n",
    "    \"id\", \"lpa_name\", \"valid_date\", \"decision\", \"status\",\n",
    "    \"application_type\", \"postcode\"\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# API CONFIG\n",
    "# =========================================================\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# =========================================================\n",
    "# BOROUGHS (canonical for lpa_name.raw)\n",
    "# =========================================================\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "START_YEAR, END_YEAR = 2010, 2025\n",
    "\n",
    "# =========================================================\n",
    "# OUTPUT\n",
    "# =========================================================\n",
    "OUT_DIR = r\"c:/Users/Jose Miguel/OneDrive/Ambiente de Trabalho/NOVA/TESE/planning_extract_core_parts\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "QA_LOG = os.path.join(OUT_DIR, \"_qa_log.csv\")\n",
    "\n",
    "# =========================================================\n",
    "# FIELDS (CORE EXTRACT)\n",
    "# =========================================================\n",
    "FIELDS = [\n",
    "    \"id\", \"lpa_app_no\", \"case_reference\", \"planning_portal_reference\",\n",
    "    \"lpa_name\", \"ward\", \"postcode\", \"site_name\", \"site_number\", \"uprn\",\n",
    "    \"received_date\", \"valid_date\", \"decision_date\", \"lapsed_date\", \"last_updated\",\n",
    "    \"application_type\", \"application_type_full\", \"development_type\",\n",
    "    \"decision\", \"status\", \"decision_process\",\n",
    "    \"centroid_easting\", \"centroid_northing\", \"centroid_latitude\", \"centroid_longitude\",\n",
    "    \"latitude\", \"longitude\"\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# QA HELPERS\n",
    "# =========================================================\n",
    "def _to_dt(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse mixed date formats for QA only. Does not affect saved file.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(series, errors=\"coerce\", utc=True, dayfirst=True, format=\"mixed\")\n",
    "    except TypeError:\n",
    "        return pd.to_datetime(series, errors=\"coerce\", utc=True, dayfirst=True)\n",
    "\n",
    "def qa_chunk(df: pd.DataFrame, borough: str, year: int) -> dict:\n",
    "    outfile = f\"planning_core_{borough.replace('&','and').replace(' ','_')}_{year}.csv\"\n",
    "    report = {\"borough\": borough, \"year\": int(year), \"outfile\": outfile}\n",
    "\n",
    "    n = len(df)\n",
    "    report[\"rows\"] = int(n)\n",
    "\n",
    "    # IDs\n",
    "    if \"id\" in df.columns:\n",
    "        id_col = df[\"id\"]\n",
    "        report[\"id_missing_pct\"] = float(id_col.isna().mean())\n",
    "        report[\"id_distinct\"] = int(id_col.nunique(dropna=True))\n",
    "        report[\"id_dup_rows\"] = int(id_col.dropna().duplicated().sum())\n",
    "    else:\n",
    "        report[\"id_missing_pct\"] = 1.0\n",
    "        report[\"id_distinct\"] = 0\n",
    "        report[\"id_dup_rows\"] = 0\n",
    "\n",
    "    # Missingness for core columns\n",
    "    for c in KEY_COLS:\n",
    "        report[f\"{c}_missing_pct\"] = float(df[c].isna().mean()) if c in df.columns else 1.0\n",
    "\n",
    "    # Date sanity + year check (QA only)\n",
    "    if \"valid_date\" in df.columns and n > 0:\n",
    "        vd = _to_dt(df[\"valid_date\"])\n",
    "        report[\"valid_date_min\"] = str(vd.min()) if vd.notna().any() else \"\"\n",
    "        report[\"valid_date_max\"] = str(vd.max()) if vd.notna().any() else \"\"\n",
    "        report[\"valid_date_parse_fail_pct\"] = float(vd.isna().mean())\n",
    "\n",
    "        parsed = vd.dropna()\n",
    "        if len(parsed) > 0:\n",
    "            report[\"valid_date_outside_year_pct\"] = float((parsed.dt.year != year).mean())\n",
    "        else:\n",
    "            report[\"valid_date_outside_year_pct\"] = 0.0\n",
    "    else:\n",
    "        report[\"valid_date_min\"] = \"\"\n",
    "        report[\"valid_date_max\"] = \"\"\n",
    "        report[\"valid_date_parse_fail_pct\"] = 1.0\n",
    "        report[\"valid_date_outside_year_pct\"] = 0.0\n",
    "\n",
    "    # Flags\n",
    "    flags = []\n",
    "\n",
    "    if n == 0:\n",
    "        flags.append(\"EMPTY_CHUNK\")\n",
    "\n",
    "    # tiny chunks are suspicious (tune threshold if you want)\n",
    "    if 0 < n < 50:\n",
    "        flags.append(\"LOW_VOLUME_LT50\")\n",
    "\n",
    "    if report[\"id_missing_pct\"] > 0.001:\n",
    "        flags.append(\"MISSING_IDS\")\n",
    "\n",
    "    if report[\"id_dup_rows\"] > 0:\n",
    "        flags.append(\"DUPLICATE_IDS\")\n",
    "\n",
    "    # valid_date should exist (you filter on it)\n",
    "    if report.get(\"valid_date_missing_pct\", 1.0) > 0.01:\n",
    "        flags.append(\"VALID_DATE_MISSING_HIGH\")\n",
    "\n",
    "    if report[\"valid_date_parse_fail_pct\"] > 0.05:\n",
    "        flags.append(\"VALID_DATE_PARSE_FAIL_HIGH\")\n",
    "\n",
    "    if report[\"valid_date_outside_year_pct\"] > 0.01:\n",
    "        flags.append(\"VALID_DATE_OUTSIDE_YEAR\")\n",
    "\n",
    "    for must in [\"lpa_name\", \"status\", \"decision\"]:\n",
    "        if must not in df.columns:\n",
    "            flags.append(f\"MISSING_COL_{must}\")\n",
    "\n",
    "    report[\"flags\"] = \"|\".join(flags)\n",
    "    report[\"ok\"] = (len(flags) == 0)\n",
    "    return report\n",
    "\n",
    "def qa_log_row(row: dict) -> None:\n",
    "    \"\"\"Append QA report to QA_LOG CSV, writing header once.\"\"\"\n",
    "    file_exists = Path(QA_LOG).exists()\n",
    "\n",
    "    # Keep column order stable (prevents messy CSV headers)\n",
    "    fieldnames = list(row.keys())\n",
    "\n",
    "    with open(QA_LOG, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "def handle_qa(report: dict) -> None:\n",
    "    \"\"\"Print warnings or raise if STRICT.\"\"\"\n",
    "    if report.get(\"ok\", True):\n",
    "        return\n",
    "\n",
    "    msg = (\n",
    "        f\"[QA] {report.get('borough')} {report.get('year')} \"\n",
    "        f\"FLAGS={report.get('flags')} rows={report.get('rows')} \"\n",
    "        f\"file={report.get('outfile')}\"\n",
    "    )\n",
    "\n",
    "    if STRICT:\n",
    "        raise RuntimeError(msg)\n",
    "    else:\n",
    "        print(msg)\n",
    "\n",
    "# =========================================================\n",
    "# EXTRACTION HELPERS\n",
    "# =========================================================\n",
    "def year_iter(start: int, end: int):\n",
    "    for y in range(start, end + 1):\n",
    "        yield f\"01/01/{y}\", f\"01/01/{y+1}\", y\n",
    "\n",
    "def fetch_borough_year(session: requests.Session, borough: str, gte_date: str, lt_date: str, size: int = 1000):\n",
    "    rows, page = [], 0\n",
    "    while True:\n",
    "        query = {\n",
    "            \"query\": {\"bool\": {\"must\": [\n",
    "                {\"term\": {\"lpa_name.raw\": borough}},\n",
    "                {\"range\": {\"valid_date\": {\"gte\": gte_date, \"lt\": lt_date}}}\n",
    "            ]}},\n",
    "            \"_source\": FIELDS,\n",
    "            \"from\": page * size,\n",
    "            \"size\": size\n",
    "        }\n",
    "\n",
    "        r = session.post(API, headers=HDR, json=query, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        hits = r.json().get(\"hits\", {}).get(\"hits\", [])\n",
    "        if not hits:\n",
    "            break\n",
    "\n",
    "        rows.extend(h.get(\"_source\", {}) for h in hits)\n",
    "\n",
    "        if len(hits) < size:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.05)  # polite pacing\n",
    "\n",
    "    return rows\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    with requests.Session() as session:\n",
    "        for borough in BOROUGHS:\n",
    "            print(f\"\\n=== {borough} ===\")\n",
    "            for gte, lt, year in tqdm(list(year_iter(START_YEAR, END_YEAR)), desc=\"Years\"):\n",
    "                data = fetch_borough_year(session, borough, gte, lt)\n",
    "                print(f\"{year}: {len(data)} records\")\n",
    "\n",
    "                if not data:\n",
    "                    # Still log QA row for visibility (optional; comment out if you don't want it)\n",
    "                    df_empty = pd.DataFrame(columns=FIELDS)\n",
    "                    rep = qa_chunk(df_empty, borough, year)\n",
    "                    qa_log_row(rep)\n",
    "                    handle_qa(rep)\n",
    "                    continue\n",
    "\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                rep = qa_chunk(df, borough, year)\n",
    "                qa_log_row(rep)\n",
    "                handle_qa(rep)\n",
    "\n",
    "                tag = f\"{borough.replace('&','and').replace(' ','_')}_{year}\"\n",
    "                out_path = os.path.join(OUT_DIR, f\"planning_core_{tag}.csv\")\n",
    "                df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(\"\\nDONE.\")\n",
    "    print(f\"Parts written to: {OUT_DIR}\")\n",
    "    print(f\"QA log written to : {QA_LOG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d5389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 8)\n",
      "                 borough                                   names_used  \\\n",
      "19  Kensington & Chelsea  Kensington & Chelsea|Kensington and Chelsea   \n",
      "28                Sutton                                       Sutton   \n",
      "12  Hammersmith & Fulham  Hammersmith & Fulham|Hammersmith and Fulham   \n",
      "20              Kingston                Kingston|Kingston upon Thames   \n",
      "13              Haringey                                     Haringey   \n",
      "22              Lewisham                                     Lewisham   \n",
      "10             Greenwich                                    Greenwich   \n",
      "24                Newham                                       Newham   \n",
      "7                Croydon                                      Croydon   \n",
      "16            Hillingdon                                   Hillingdon   \n",
      "\n",
      "    total_docs  pct_has_valid_date min_valid_date min_decision_date  \\\n",
      "19       39606            0.908170     02/01/2019        02/01/1987   \n",
      "28       20275            0.814254     18/12/2017        08/08/1983   \n",
      "12       25838            0.854052     22/01/2016        06/01/1987   \n",
      "20       24234            0.879343     24/10/2014        06/01/1987   \n",
      "13       25929            0.778588     16/03/1983        14/02/1980   \n",
      "22       26358            0.815616     14/09/2010        12/01/1987   \n",
      "10       23738            0.901129     18/06/2014        19/01/1987   \n",
      "24       23071            0.859347     09/05/2005        05/01/1987   \n",
      "7        40739            0.788434     17/12/2004        10/06/1986   \n",
      "16       31736            0.866398     06/03/2001        05/01/1987   \n",
      "\n",
      "    pre2019_by_valid_date  pre2019_by_decision_date  \n",
      "19                      0                      3617  \n",
      "28                      3                      3587  \n",
      "12                      6                      3611  \n",
      "20                     10                      2845  \n",
      "13                     11                      3966  \n",
      "22                     15                      4297  \n",
      "10                     15                      2306  \n",
      "24                     15                      3037  \n",
      "7                      49                      8507  \n",
      "16                     85                      3611  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "# Optional: add aliases when QA shows weird low/zero for pre-2019\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "def borough_terms(borough: str):\n",
    "    return BOROUGH_ALIASES.get(borough, [borough])\n",
    "\n",
    "def agg_check_borough(session: requests.Session, borough: str):\n",
    "    names = borough_terms(borough)\n",
    "\n",
    "    # overall min valid_date & min decision_date, plus % with valid_date\n",
    "    q_all = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}}\n",
    "        ]}},\n",
    "        \"aggs\": {\n",
    "            \"min_valid_date\": {\"min\": {\"field\": \"valid_date\"}},\n",
    "            \"min_decision_date\": {\"min\": {\"field\": \"decision_date\"}},\n",
    "            \"has_valid_date\": {\"filter\": {\"exists\": {\"field\": \"valid_date\"}}},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # count pre-2019 by decision_date (fallback lens)\n",
    "    q_pre2019_decision = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}},\n",
    "            {\"range\": {\"decision_date\": {\"lt\": \"01/01/2019\"}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    # count pre-2019 by valid_date (your extraction lens)\n",
    "    q_pre2019_valid = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}},\n",
    "            {\"range\": {\"valid_date\": {\"lt\": \"01/01/2019\"}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    r_all = session.post(API, headers=HDR, json=q_all, timeout=60); r_all.raise_for_status()\n",
    "    r_dec = session.post(API, headers=HDR, json=q_pre2019_decision, timeout=60); r_dec.raise_for_status()\n",
    "    r_val = session.post(API, headers=HDR, json=q_pre2019_valid, timeout=60); r_val.raise_for_status()\n",
    "\n",
    "    j_all = r_all.json()\n",
    "    total = j_all[\"hits\"][\"total\"][\"value\"]\n",
    "    has_valid = j_all[\"aggregations\"][\"has_valid_date\"][\"doc_count\"]\n",
    "    pct_valid = (has_valid / total) if total else 0.0\n",
    "\n",
    "    return {\n",
    "        \"borough\": borough,\n",
    "        \"names_used\": \"|\".join(names),\n",
    "        \"total_docs\": total,\n",
    "        \"pct_has_valid_date\": pct_valid,\n",
    "        \"min_valid_date\": j_all[\"aggregations\"][\"min_valid_date\"].get(\"value_as_string\"),\n",
    "        \"min_decision_date\": j_all[\"aggregations\"][\"min_decision_date\"].get(\"value_as_string\"),\n",
    "        \"pre2019_by_valid_date\": r_val.json()[\"hits\"][\"total\"][\"value\"],\n",
    "        \"pre2019_by_decision_date\": r_dec.json()[\"hits\"][\"total\"][\"value\"],\n",
    "    }\n",
    "\n",
    "def run_checks():\n",
    "    out = []\n",
    "    with requests.Session() as session:\n",
    "        for b in BOROUGHS:\n",
    "            out.append(agg_check_borough(session, b))\n",
    "    df = pd.DataFrame(out).sort_values([\"pre2019_by_valid_date\", \"total_docs\"], ascending=[True, False])\n",
    "    return df\n",
    "\n",
    "df_checks = run_checks()\n",
    "print(df_checks.shape)\n",
    "print(df_checks.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebad2268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total boroughs: 33\n",
      "Trainable 2019-2024 (by valid_date): 15\n",
      "NOT trainable 2019-2024 (by valid_date): 18\n",
      "\n",
      "NOT trainable list:\n",
      "                 borough min_valid_date  pre2019_by_valid_date  \\\n",
      "31            Wandsworth     01/12/1999                    231   \n",
      "19  Kensington & Chelsea     02/01/2019                      0   \n",
      "23                Merton     02/05/1986                    457   \n",
      "11               Hackney     03/11/1994                     91   \n",
      "16            Hillingdon     06/03/2001                     85   \n",
      "32           Westminster     08/08/2002                    189   \n",
      "24                Newham     09/05/2005                     15   \n",
      "9                Enfield     11/02/2004                    213   \n",
      "15              Havering     14/02/2000                    106   \n",
      "22              Lewisham     14/09/2010                     15   \n",
      "13              Haringey     16/03/1983                     11   \n",
      "14                Harrow     16/04/1992                    194   \n",
      "7                Croydon     17/12/2004                     49   \n",
      "10             Greenwich     18/06/2014                     15   \n",
      "28                Sutton     18/12/2017                      3   \n",
      "12  Hammersmith & Fulham     22/01/2016                      6   \n",
      "0     Barking & Dagenham     23/07/1998                    226   \n",
      "20              Kingston     24/10/2014                     10   \n",
      "\n",
      "    pct_has_valid_date  \n",
      "31            0.755141  \n",
      "19            0.908170  \n",
      "23            0.794249  \n",
      "11            0.697780  \n",
      "16            0.866398  \n",
      "32            0.886972  \n",
      "24            0.859347  \n",
      "9             0.868104  \n",
      "15            0.878568  \n",
      "22            0.815616  \n",
      "13            0.778588  \n",
      "14            0.791556  \n",
      "7             0.788434  \n",
      "10            0.901129  \n",
      "28            0.814254  \n",
      "12            0.854052  \n",
      "0             0.673538  \n",
      "20            0.879343  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_checks already exists from your run_checks()\n",
    "\n",
    "# Parse min_valid_date safely (dayfirst because your strings look dd/mm/yyyy)\n",
    "df = df_checks.copy()\n",
    "df[\"min_valid_date_dt\"] = pd.to_datetime(df[\"min_valid_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# --- Define trainability criteria (edit thresholds if needed)\n",
    "MIN_PRE2019 = 500  # <-- change to 1000 if you want stricter\n",
    "HAS_2010 = df[\"min_valid_date_dt\"] <= pd.Timestamp(\"2019-12-31\")\n",
    "HAS_ENOUGH_PRE2019 = df[\"pre2019_by_valid_date\"].fillna(0).astype(int) >= MIN_PRE2019\n",
    "\n",
    "df[\"trainable_2010_2019_by_valid_date\"] = HAS_2010 & HAS_ENOUGH_PRE2019\n",
    "\n",
    "total = len(df)\n",
    "trainable = int(df[\"trainable_2010_2019_by_valid_date\"].sum())\n",
    "not_trainable = total - trainable\n",
    "\n",
    "print(\"Total boroughs:\", total)\n",
    "print(\"Trainable 2019-2024 (by valid_date):\", trainable)\n",
    "print(\"NOT trainable 2019-2024 (by valid_date):\", not_trainable)\n",
    "\n",
    "print(\"\\nNOT trainable list:\")\n",
    "print(df.loc[~df[\"trainable_2010_2019_by_valid_date\"], [\"borough\",\"min_valid_date\",\"pre2019_by_valid_date\",\"pct_has_valid_date\"]]\n",
    "      .sort_values([\"min_valid_date\",\"pre2019_by_valid_date\"], ascending=[True, True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ed94f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Boroughs: 100%|██████████| 33/33 [00:20<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total boroughs: 33\n",
      "Trainable (year-coverage rule): 5\n",
      "NOT trainable: 28\n",
      "\n",
      "NOT trainable list (worst first):\n",
      "                 borough  years_ge_min  total_2010_2019  min_year_count  \\\n",
      "31            Wandsworth             0              226               3   \n",
      "14                Harrow             0              233               6   \n",
      "0     Barking & Dagenham             1              850               3   \n",
      "11               Hackney             1             2193               2   \n",
      "28                Sutton             1             2444               0   \n",
      "23                Merton             1             2660              12   \n",
      "20              Kingston             1             2775               0   \n",
      "13              Haringey             1             3065               0   \n",
      "2                 Bexley             1             3121              21   \n",
      "10             Greenwich             1             3125               0   \n",
      "15              Havering             1             3252               0   \n",
      "12  Hammersmith & Fulham             1             3329               0   \n",
      "22              Lewisham             1             3402               0   \n",
      "24                Newham             1             3419               0   \n",
      "27             Southwark             1             3830               1   \n",
      "9                Enfield             1             4133               0   \n",
      "16            Hillingdon             1             4136               0   \n",
      "7                Croydon             1             5008               0   \n",
      "19  Kensington & Chelsea             1             5173               0   \n",
      "26              Richmond             1             5474              46   \n",
      "32           Westminster             1             9680               2   \n",
      "6         City of London             2             1981              34   \n",
      "30        Waltham Forest             2             3048               1   \n",
      "3                  Brent             2             8873               0   \n",
      "21               Lambeth             3             4910              49   \n",
      "25             Redbridge             3             6680              26   \n",
      "29         Tower Hamlets             4             9377               0   \n",
      "4                Bromley             5             7095              80   \n",
      "\n",
      "    max_year_count  \n",
      "31              75  \n",
      "14              81  \n",
      "0              685  \n",
      "11            2150  \n",
      "28            2441  \n",
      "23            2352  \n",
      "20            2765  \n",
      "13            3058  \n",
      "2             2738  \n",
      "10            3110  \n",
      "15            3182  \n",
      "12            3323  \n",
      "22            3387  \n",
      "24            3405  \n",
      "27            3762  \n",
      "9             3937  \n",
      "16            4077  \n",
      "7             4962  \n",
      "19            5173  \n",
      "26            4684  \n",
      "32            9511  \n",
      "6             1123  \n",
      "30            2317  \n",
      "3             4653  \n",
      "21            3930  \n",
      "25            4681  \n",
      "29            3069  \n",
      "4             5398  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "def names_for(b):\n",
    "    return BOROUGH_ALIASES.get(b, [b])\n",
    "\n",
    "def count_year(session, borough, year):\n",
    "    gte = f\"01/01/{year}\"\n",
    "    lt  = f\"01/01/{year+1}\"\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names_for(borough)}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": gte, \"lt\": lt}}}\n",
    "        ]}}\n",
    "    }\n",
    "    r = session.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "# ---- define \"trainable\" rules\n",
    "YEARS = list(range(2010, 2020))\n",
    "MIN_ROWS_PER_YEAR = 200     # <-- adjust (100/200/500). 200 is a sane starting point.\n",
    "MIN_YEARS_PRESENT = 8       # need at least 8/10 non-trivial years\n",
    "MIN_TOTAL_ROWS = 3000       # optional: overall mass in 2010-2019\n",
    "\n",
    "rows = []\n",
    "with requests.Session() as session:\n",
    "    for b in tqdm(BOROUGHS, desc=\"Boroughs\"):\n",
    "        counts = []\n",
    "        for y in YEARS:\n",
    "            n = count_year(session, b, y)\n",
    "            counts.append(n)\n",
    "        total = sum(counts)\n",
    "        years_ok = sum(1 for n in counts if n >= MIN_ROWS_PER_YEAR)\n",
    "\n",
    "        rows.append({\n",
    "            \"borough\": b,\n",
    "            \"total_2010_2019\": total,\n",
    "            \"years_ge_min\": years_ok,\n",
    "            \"min_year_count\": min(counts),\n",
    "            \"max_year_count\": max(counts),\n",
    "            \"counts_2010_2019\": counts\n",
    "        })\n",
    "\n",
    "df_train = pd.DataFrame(rows)\n",
    "df_train[\"trainable\"] = (\n",
    "    (df_train[\"years_ge_min\"] >= MIN_YEARS_PRESENT) &\n",
    "    (df_train[\"total_2010_2019\"] >= MIN_TOTAL_ROWS)\n",
    ")\n",
    "\n",
    "print(\"Total boroughs:\", len(df_train))\n",
    "print(\"Trainable (year-coverage rule):\", int(df_train[\"trainable\"].sum()))\n",
    "print(\"NOT trainable:\", int((~df_train[\"trainable\"]).sum()))\n",
    "\n",
    "print(\"\\nNOT trainable list (worst first):\")\n",
    "print(df_train.loc[~df_train[\"trainable\"], [\"borough\",\"years_ge_min\",\"total_2010_2019\",\"min_year_count\",\"max_year_count\"]]\n",
    "      .sort_values([\"years_ge_min\",\"total_2010_2019\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc87aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable boroughs:\n",
      "      borough  years_ge_min  total_2010_2019  min_year_count  max_year_count\n",
      "18  Islington            10            40326            3319            4529\n",
      "1      Barnet            10            13916             236            7258\n",
      "8      Ealing             9            44017             199            6345\n",
      "17   Hounslow             8            17514             131            4173\n",
      "5      Camden             8             4126             150            1624\n"
     ]
    }
   ],
   "source": [
    "# df_train is from your year-coverage run\n",
    "trainable = df_train[df_train[\"trainable\"]].copy()\n",
    "print(\"Trainable boroughs:\")\n",
    "print(trainable[[\"borough\",\"years_ge_min\",\"total_2010_2019\",\"min_year_count\",\"max_year_count\"]]\n",
    "      .sort_values([\"years_ge_min\",\"total_2010_2019\"], ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df5e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnet years_ge_min= 10 total= 13916\n",
      "2010 236\n",
      "2011 283\n",
      "2012 294\n",
      "2013 464\n",
      "2014 601\n",
      "2015 888\n",
      "2016 1078\n",
      "2017 1272\n",
      "2018 1542\n",
      "2019 7258\n",
      "Camden years_ge_min= 8 total= 4126\n",
      "2010 180\n",
      "2011 150\n",
      "2012 201\n",
      "2013 353\n",
      "2014 388\n",
      "2015 387\n",
      "2016 343\n",
      "2017 253\n",
      "2018 247\n",
      "2019 1624\n",
      "Westminster years_ge_min= 1 total= 9680\n",
      "2010 2\n",
      "2011 4\n",
      "2012 2\n",
      "2013 3\n",
      "2014 10\n",
      "2015 12\n",
      "2016 19\n",
      "2017 37\n",
      "2018 80\n",
      "2019 9511\n"
     ]
    }
   ],
   "source": [
    "def show_counts(borough_name):\n",
    "    row = df_train[df_train[\"borough\"] == borough_name].iloc[0]\n",
    "    counts = row[\"counts_2010_2019\"]\n",
    "    years = list(range(2010, 2020))\n",
    "    print(borough_name, \"years_ge_min=\", row[\"years_ge_min\"], \"total=\", row[\"total_2010_2019\"])\n",
    "    for y, n in zip(years, counts):\n",
    "        print(y, n)\n",
    "\n",
    "# Try a few\n",
    "show_counts(\"Barnet\")\n",
    "show_counts(\"Camden\")\n",
    "show_counts(\"Westminster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72816bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Boroughs: 100%|██████████| 33/33 [00:21<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable (decision_date) 2010-2019: 11\n",
      "NOT trainable: 22\n",
      "\n",
      "Trainable boroughs (decision_date):\n",
      "        borough  years_ge_min_decision_date  total_2010_2019_decision_date  \\\n",
      "8        Ealing                          10                          44961   \n",
      "18    Islington                          10                          36631   \n",
      "17     Hounslow                          10                          17361   \n",
      "1        Barnet                          10                          15015   \n",
      "32  Westminster                          10                          11894   \n",
      "4       Bromley                          10                           8112   \n",
      "7       Croydon                          10                           8076   \n",
      "5        Camden                          10                           4752   \n",
      "11      Hackney                          10                           4680   \n",
      "31   Wandsworth                          10                           3573   \n",
      "21      Lambeth                           9                           5949   \n",
      "\n",
      "    min_year_count_decision_date  max_year_count_decision_date  \n",
      "8                            347                          6483  \n",
      "18                          2848                          4296  \n",
      "17                           227                          3790  \n",
      "1                            474                          6451  \n",
      "32                           326                          7992  \n",
      "4                            274                          4646  \n",
      "7                            347                          4202  \n",
      "5                            284                          1175  \n",
      "11                           260                          1888  \n",
      "31                           279                           473  \n",
      "21                           194                          3285  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "def names_for(b):\n",
    "    return BOROUGH_ALIASES.get(b, [b])\n",
    "\n",
    "def count_year_by_field(session, borough, year, field):\n",
    "    gte = f\"01/01/{year}\"\n",
    "    lt  = f\"01/01/{year+1}\"\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names_for(borough)}},\n",
    "            {\"range\": {field: {\"gte\": gte, \"lt\": lt}}}\n",
    "        ]}}\n",
    "    }\n",
    "    r = session.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "YEARS = list(range(2010, 2020))\n",
    "MIN_ROWS_PER_YEAR = 200\n",
    "MIN_YEARS_PRESENT = 8\n",
    "MIN_TOTAL_ROWS = 3000\n",
    "\n",
    "rows = []\n",
    "with requests.Session() as session:\n",
    "    for b in tqdm(BOROUGHS, desc=\"Boroughs\"):\n",
    "        counts = [count_year_by_field(session, b, y, \"decision_date\") for y in YEARS]\n",
    "        total = sum(counts)\n",
    "        years_ok = sum(1 for n in counts if n >= MIN_ROWS_PER_YEAR)\n",
    "        rows.append({\n",
    "            \"borough\": b,\n",
    "            \"total_2010_2019_decision_date\": total,\n",
    "            \"years_ge_min_decision_date\": years_ok,\n",
    "            \"min_year_count_decision_date\": min(counts),\n",
    "            \"max_year_count_decision_date\": max(counts),\n",
    "            \"counts_2010_2019_decision_date\": counts\n",
    "        })\n",
    "\n",
    "df_dec = pd.DataFrame(rows)\n",
    "df_dec[\"trainable_decision_date\"] = (\n",
    "    (df_dec[\"years_ge_min_decision_date\"] >= MIN_YEARS_PRESENT) &\n",
    "    (df_dec[\"total_2010_2019_decision_date\"] >= MIN_TOTAL_ROWS)\n",
    ")\n",
    "\n",
    "print(\"Trainable (decision_date) 2010-2019:\", int(df_dec[\"trainable_decision_date\"].sum()))\n",
    "print(\"NOT trainable:\", int((~df_dec[\"trainable_decision_date\"]).sum()))\n",
    "\n",
    "print(\"\\nTrainable boroughs (decision_date):\")\n",
    "print(df_dec[df_dec[\"trainable_decision_date\"]][\n",
    "    [\"borough\",\"years_ge_min_decision_date\",\"total_2010_2019_decision_date\",\n",
    "     \"min_year_count_decision_date\",\"max_year_count_decision_date\"]\n",
    "].sort_values([\"years_ge_min_decision_date\",\"total_2010_2019_decision_date\"], ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e25d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainable boroughs: 100%|██████████| 11/11 [00:17<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             decision_count  bad_pre1990\n",
      "borough                                 \n",
      "Ealing                44961            0\n",
      "Islington             36631            0\n",
      "Hounslow              17361            0\n",
      "Barnet                15015            0\n",
      "Westminster           11894            0\n",
      "Bromley                8112            0\n",
      "Croydon                8076            0\n",
      "Lambeth                5949            0\n",
      "Camden                 4752            0\n",
      "Hackney                4680            0\n",
      "Wandsworth             3573            0\n",
      "\n",
      "Worst borough-years by bad_pre1990_pct:\n",
      "    borough  year  decision_count  bad_pre1990  bad_pre1990_pct\n",
      "0    Ealing  2010             448            0              0.0\n",
      "69  Croydon  2019            4202            0              0.0\n",
      "80  Hackney  2010             327            0              0.0\n",
      "79   Camden  2019            1175            0              0.0\n",
      "78   Camden  2018             284            0              0.0\n",
      "77   Camden  2017             339            0              0.0\n",
      "76   Camden  2016             404            0              0.0\n",
      "75   Camden  2015             477            0              0.0\n",
      "74   Camden  2014             438            0              0.0\n",
      "73   Camden  2013             429            0              0.0\n",
      "72   Camden  2012             373            0              0.0\n",
      "71   Camden  2011             387            0              0.0\n",
      "70   Camden  2010             446            0              0.0\n",
      "68  Croydon  2018             579            0              0.0\n",
      "82  Hackney  2012             260            0              0.0\n",
      "67  Croydon  2017             573            0              0.0\n",
      "66  Croydon  2016             459            0              0.0\n",
      "65  Croydon  2015             425            0              0.0\n",
      "64  Croydon  2014             401            0              0.0\n",
      "63  Croydon  2013             380            0              0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "TRAINABLE = [\n",
    "    \"Ealing\",\"Islington\",\"Hounslow\",\"Barnet\",\"Westminster\",\"Bromley\",\n",
    "    \"Croydon\",\"Camden\",\"Hackney\",\"Wandsworth\",\"Lambeth\"\n",
    "]\n",
    "\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "def names_for(b):\n",
    "    return BOROUGH_ALIASES.get(b, [b])\n",
    "\n",
    "def agg_year(session, borough, year):\n",
    "    gte = f\"01/01/{year}\"\n",
    "    lt  = f\"01/01/{year+1}\"\n",
    "\n",
    "    # total decisions in that year\n",
    "    q_total = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names_for(borough)}},\n",
    "            {\"range\": {\"decision_date\": {\"gte\": gte, \"lt\": lt}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    # suspicious early decisions (before 1990) — should basically be 0\n",
    "    q_bad = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names_for(borough)}},\n",
    "            {\"range\": {\"decision_date\": {\"gte\": gte, \"lt\": lt}}},\n",
    "            {\"range\": {\"decision_date\": {\"lt\": \"01/01/1990\"}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    rt = session.post(API, headers=HDR, json=q_total, timeout=60); rt.raise_for_status()\n",
    "    rb = session.post(API, headers=HDR, json=q_bad, timeout=60);   rb.raise_for_status()\n",
    "\n",
    "    total = rt.json()[\"hits\"][\"total\"][\"value\"]\n",
    "    bad   = rb.json()[\"hits\"][\"total\"][\"value\"]\n",
    "    return total, bad\n",
    "\n",
    "rows = []\n",
    "with requests.Session() as session:\n",
    "    for b in tqdm(TRAINABLE, desc=\"Trainable boroughs\"):\n",
    "        for y in range(2010, 2020):\n",
    "            total, bad = agg_year(session, b, y)\n",
    "            rows.append({\"borough\": b, \"year\": y, \"decision_count\": total, \"bad_pre1990\": bad})\n",
    "\n",
    "df_q = pd.DataFrame(rows)\n",
    "df_q[\"bad_pre1990_pct\"] = df_q[\"bad_pre1990\"] / df_q[\"decision_count\"].replace(0, pd.NA)\n",
    "\n",
    "print(df_q.groupby(\"borough\")[[\"decision_count\",\"bad_pre1990\"]].sum().sort_values(\"decision_count\", ascending=False))\n",
    "print(\"\\nWorst borough-years by bad_pre1990_pct:\")\n",
    "print(df_q.sort_values(\"bad_pre1990_pct\", ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c2bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Boroughs: 100%|██████████| 11/11 [00:21<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         borough  year  total  missing_decision  missing_status  \\\n",
      "22      Hounslow  2012    343                26               0   \n",
      "50       Bromley  2010    307                19               1   \n",
      "20      Hounslow  2010    257                11               0   \n",
      "101      Lambeth  2011    194                 7               0   \n",
      "60       Croydon  2010    362                13               0   \n",
      "100      Lambeth  2010    251                 8               0   \n",
      "51       Bromley  2011    274                 7               0   \n",
      "31        Barnet  2011    474                12               0   \n",
      "23      Hounslow  2013   1082                27               0   \n",
      "56       Bromley  2016    470                 9               0   \n",
      "48   Westminster  2018    326                 6              22   \n",
      "1         Ealing  2011    347                 6               0   \n",
      "52       Bromley  2012    291                 5               1   \n",
      "63       Croydon  2013    380                 6               0   \n",
      "70        Camden  2010    446                 7               0   \n",
      "62       Croydon  2012    347                 5               0   \n",
      "54       Bromley  2014    348                 5               0   \n",
      "21      Hounslow  2011    227                 3               0   \n",
      "102      Lambeth  2012    311                 4               0   \n",
      "103      Lambeth  2013    315                 4               0   \n",
      "\n",
      "     missing_decision_pct  missing_status_pct  \n",
      "22               0.075802            0.000000  \n",
      "50               0.061889            0.003257  \n",
      "20               0.042802            0.000000  \n",
      "101              0.036082            0.000000  \n",
      "60               0.035912            0.000000  \n",
      "100              0.031873            0.000000  \n",
      "51               0.025547            0.000000  \n",
      "31               0.025316            0.000000  \n",
      "23               0.024954            0.000000  \n",
      "56               0.019149            0.000000  \n",
      "48               0.018405            0.067485  \n",
      "1                0.017291            0.000000  \n",
      "52               0.017182            0.003436  \n",
      "63               0.015789            0.000000  \n",
      "70               0.015695            0.000000  \n",
      "62               0.014409            0.000000  \n",
      "54               0.014368            0.000000  \n",
      "21               0.013216            0.000000  \n",
      "102              0.012862            0.000000  \n",
      "103              0.012698            0.000000  \n",
      "\n",
      "By borough (avg pct):\n",
      "             missing_decision_pct  missing_status_pct\n",
      "borough                                              \n",
      "Hounslow                 0.017711            0.000000\n",
      "Bromley                  0.017370            0.013609\n",
      "Lambeth                  0.011847            0.000030\n",
      "Croydon                  0.011716            0.000048\n",
      "Barnet                   0.007139            0.000249\n",
      "Ealing                   0.006324            0.000050\n",
      "Westminster              0.006054            0.017961\n",
      "Camden                   0.003665            0.000000\n",
      "Wandsworth               0.002125            0.000000\n",
      "Hackney                  0.000698            0.000000\n",
      "Islington                0.000506            0.000051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "TRAINABLE = [\n",
    "    \"Ealing\",\"Islington\",\"Hounslow\",\"Barnet\",\"Westminster\",\n",
    "    \"Bromley\",\"Croydon\",\"Camden\",\"Hackney\",\"Wandsworth\",\"Lambeth\"\n",
    "]\n",
    "\n",
    "def count_query(session, borough, year, extra_must=None):\n",
    "    gte = f\"01/01/{year}\"\n",
    "    lt  = f\"01/01/{year+1}\"\n",
    "    must = [\n",
    "        {\"term\": {\"lpa_name.raw\": borough}},\n",
    "        {\"range\": {\"decision_date\": {\"gte\": gte, \"lt\": lt}}}\n",
    "    ]\n",
    "    if extra_must:\n",
    "        must += extra_must\n",
    "\n",
    "    q = {\"size\": 0, \"track_total_hits\": True, \"query\": {\"bool\": {\"must\": must}}}\n",
    "    r = session.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "rows = []\n",
    "with requests.Session() as session:\n",
    "    for b in tqdm(TRAINABLE, desc=\"Boroughs\"):\n",
    "        for y in range(2010, 2020):\n",
    "            total = count_query(session, b, y)\n",
    "            missing_decision = count_query(session, b, y, extra_must=[{\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision\"}}]}}])\n",
    "            missing_status   = count_query(session, b, y, extra_must=[{\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"status\"}}]}}])\n",
    "\n",
    "            rows.append({\n",
    "                \"borough\": b, \"year\": y, \"total\": total,\n",
    "                \"missing_decision\": missing_decision,\n",
    "                \"missing_status\": missing_status,\n",
    "                \"missing_decision_pct\": (missing_decision / total) if total else None,\n",
    "                \"missing_status_pct\": (missing_status / total) if total else None,\n",
    "            })\n",
    "\n",
    "df_miss = pd.DataFrame(rows)\n",
    "print(df_miss.sort_values([\"missing_decision_pct\"], ascending=False).head(20))\n",
    "print(\"\\nBy borough (avg pct):\")\n",
    "print(df_miss.groupby(\"borough\")[[\"missing_decision_pct\",\"missing_status_pct\"]].mean().sort_values(\"missing_decision_pct\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f21aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "# Optional: aliases when the API has inconsistent lpa_name values\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "TRAIN_START = \"01/01/2019\"\n",
    "TRAIN_END_EXCL = \"01/01/2025\"   # train: [2019-01-01, 2025-01-01)\n",
    "TEST_START = \"01/01/2025\"\n",
    "TEST_END_EXCL = \"01/01/2026\"    # test:  [2025-01-01, 2026-01-01)\n",
    "\n",
    "# \"Trainable\" thresholds (tune if you want)\n",
    "MIN_TRAIN = 5000   # minimum decisions in 2019-2024\n",
    "MIN_TEST = 500     # minimum decisions in 2025\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def borough_terms(borough: str):\n",
    "    return BOROUGH_ALIASES.get(borough, [borough])\n",
    "\n",
    "def agg_check_borough(session: requests.Session, borough: str) -> dict:\n",
    "    names = borough_terms(borough)\n",
    "\n",
    "    # overall min valid_date & min decision_date, plus % coverage\n",
    "    q_all = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}}\n",
    "        ]}},\n",
    "        \"aggs\": {\n",
    "            \"min_valid_date\": {\"min\": {\"field\": \"valid_date\"}},\n",
    "            \"min_decision_date\": {\"min\": {\"field\": \"decision_date\"}},\n",
    "            \"has_valid_date\": {\"filter\": {\"exists\": {\"field\": \"valid_date\"}}},\n",
    "            \"has_decision_date\": {\"filter\": {\"exists\": {\"field\": \"decision_date\"}}},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # TRAIN: 2019-2024 by decision_date\n",
    "    q_train = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}},\n",
    "            {\"range\": {\"decision_date\": {\"gte\": TRAIN_START, \"lt\": TRAIN_END_EXCL}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    # TEST: 2025 by decision_date\n",
    "    q_test = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}},\n",
    "            {\"range\": {\"decision_date\": {\"gte\": TEST_START, \"lt\": TEST_END_EXCL}}}\n",
    "        ]}}\n",
    "    }\n",
    "\n",
    "    r_all = session.post(API, headers=HDR, json=q_all, timeout=60); r_all.raise_for_status()\n",
    "    r_train = session.post(API, headers=HDR, json=q_train, timeout=60); r_train.raise_for_status()\n",
    "    r_test = session.post(API, headers=HDR, json=q_test, timeout=60); r_test.raise_for_status()\n",
    "\n",
    "    j_all = r_all.json()\n",
    "    total = j_all[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "    has_valid = j_all[\"aggregations\"][\"has_valid_date\"][\"doc_count\"]\n",
    "    has_decision = j_all[\"aggregations\"][\"has_decision_date\"][\"doc_count\"]\n",
    "\n",
    "    pct_valid = (has_valid / total) if total else 0.0\n",
    "    pct_decision = (has_decision / total) if total else 0.0\n",
    "\n",
    "    return {\n",
    "        \"borough\": borough,\n",
    "        \"names_used\": \"|\".join(names),\n",
    "        \"total_docs\": int(total),\n",
    "        \"pct_has_valid_date\": float(pct_valid),\n",
    "        \"pct_has_decision_date\": float(pct_decision),\n",
    "        \"min_valid_date\": j_all[\"aggregations\"][\"min_valid_date\"].get(\"value_as_string\"),\n",
    "        \"min_decision_date\": j_all[\"aggregations\"][\"min_decision_date\"].get(\"value_as_string\"),\n",
    "        \"train_2019_2024_by_decision_date\": int(r_train.json()[\"hits\"][\"total\"][\"value\"]),\n",
    "        \"test_2025_by_decision_date\": int(r_test.json()[\"hits\"][\"total\"][\"value\"]),\n",
    "    }\n",
    "\n",
    "def run_checks():\n",
    "    out = []\n",
    "    with requests.Session() as session:\n",
    "        for b in BOROUGHS:\n",
    "            out.append(agg_check_borough(session, b))\n",
    "\n",
    "    df = pd.DataFrame(out).sort_values(\n",
    "        [\"train_2019_2024_by_decision_date\", \"test_2025_by_decision_date\", \"total_docs\"],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8be910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 9)\n",
      "                 borough                                   names_used  \\\n",
      "0     Barking & Dagenham      Barking & Dagenham|Barking and Dagenham   \n",
      "6         City of London                               City of London   \n",
      "14                Harrow                                       Harrow   \n",
      "5                 Camden                                       Camden   \n",
      "11               Hackney                                      Hackney   \n",
      "23                Merton                                       Merton   \n",
      "31            Wandsworth                                   Wandsworth   \n",
      "29         Tower Hamlets                                Tower Hamlets   \n",
      "28                Sutton                                       Sutton   \n",
      "12  Hammersmith & Fulham  Hammersmith & Fulham|Hammersmith and Fulham   \n",
      "\n",
      "    total_docs  pct_has_valid_date  pct_has_decision_date min_valid_date  \\\n",
      "0        14069            0.673538               0.665719     23/07/1998   \n",
      "6        10620            0.854896               0.922505     07/01/2004   \n",
      "14       18427            0.791556               0.700331     16/04/1992   \n",
      "5        22988            0.908430               0.861536     28/03/1949   \n",
      "11       23334            0.697780               0.828491     03/11/1994   \n",
      "23       19684            0.794249               0.963016     02/05/1986   \n",
      "31       32582            0.755141               0.787091     01/12/1999   \n",
      "29       27712            0.828125               0.950924     26/05/2010   \n",
      "28       20275            0.814254               0.978150     18/12/2017   \n",
      "12       25838            0.854052               0.846273     22/01/2016   \n",
      "\n",
      "   min_decision_date  train_2019_2024_by_decision_date  \\\n",
      "0         05/01/1948                              4595   \n",
      "6         08/01/1987                              5918   \n",
      "14        15/01/1987                              8672   \n",
      "5         25/10/1962                             10838   \n",
      "11        14/01/1987                             12027   \n",
      "23        27/10/1983                             12950   \n",
      "31        17/08/1949                             13461   \n",
      "29        06/01/1987                             14277   \n",
      "28        08/08/1983                             14359   \n",
      "12        06/01/1987                             15707   \n",
      "\n",
      "    test_2025_by_decision_date  \n",
      "0                          265  \n",
      "6                         1391  \n",
      "14                         843  \n",
      "5                         1458  \n",
      "11                        1123  \n",
      "23                        1781  \n",
      "31                        4880  \n",
      "29                        1917  \n",
      "28                        1874  \n",
      "12                        2517  \n"
     ]
    }
   ],
   "source": [
    "df_checks = run_checks()\n",
    "print(df_checks.shape)\n",
    "print(df_checks.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050cb6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total boroughs: 33\n",
      "Trainable (decision_date) train 2019-2024 + test 2025: 32\n",
      "NOT trainable: 1\n",
      "\n",
      "NOT trainable list (lowest volumes first):\n",
      "              borough min_decision_date  train_2019_2024_by_decision_date  \\\n",
      "0  Barking & Dagenham        05/01/1948                              4595   \n",
      "\n",
      "   test_2025_by_decision_date  pct_has_decision_date  \n",
      "0                         265               0.665719  \n",
      "\n",
      "Trainable boroughs:\n",
      "                 borough  train_2019_2024_by_decision_date  \\\n",
      "32           Westminster                             47231   \n",
      "1                 Barnet                             39817   \n",
      "4                Bromley                             30688   \n",
      "8                 Ealing                             30410   \n",
      "19  Kensington & Chelsea                             30143   \n",
      "7                Croydon                             27834   \n",
      "26              Richmond                             27121   \n",
      "25             Redbridge                             25332   \n",
      "16            Hillingdon                             23901   \n",
      "3                  Brent                             23008   \n",
      "17              Hounslow                             23000   \n",
      "21               Lambeth                             22830   \n",
      "9                Enfield                             22184   \n",
      "27             Southwark                             21975   \n",
      "15              Havering                             19869   \n",
      "30        Waltham Forest                             19538   \n",
      "22              Lewisham                             18638   \n",
      "20              Kingston                             17898   \n",
      "18             Islington                             17863   \n",
      "10             Greenwich                             17552   \n",
      "13              Haringey                             16865   \n",
      "24                Newham                             16839   \n",
      "2                 Bexley                             15917   \n",
      "12  Hammersmith & Fulham                             15707   \n",
      "28                Sutton                             14359   \n",
      "29         Tower Hamlets                             14277   \n",
      "31            Wandsworth                             13461   \n",
      "23                Merton                             12950   \n",
      "11               Hackney                             12027   \n",
      "5                 Camden                             10838   \n",
      "14                Harrow                              8672   \n",
      "6         City of London                              5918   \n",
      "\n",
      "    test_2025_by_decision_date  pct_has_decision_date  \n",
      "32                        8601               0.951446  \n",
      "1                         5859               0.975612  \n",
      "4                         2244               0.967396  \n",
      "8                         1784               0.962120  \n",
      "19                        4483               0.965586  \n",
      "7                         3556               0.980019  \n",
      "26                        3207               0.945297  \n",
      "25                        3668               0.961154  \n",
      "16                        3206               0.969057  \n",
      "3                         3121               0.972343  \n",
      "17                        2282               0.968676  \n",
      "21                        3520               0.951501  \n",
      "9                         3849               0.969425  \n",
      "27                        3431               0.961939  \n",
      "15                        2814               0.971497  \n",
      "30                         999               0.940802  \n",
      "22                        1787               0.937932  \n",
      "20                        2878               0.975489  \n",
      "18                        2454               0.909678  \n",
      "10                        3071               0.966509  \n",
      "13                        2360               0.894982  \n",
      "24                        2435               0.967492  \n",
      "2                         2341               0.951950  \n",
      "12                        2517               0.846273  \n",
      "28                        1874               0.978150  \n",
      "29                        1917               0.950924  \n",
      "31                        4880               0.787091  \n",
      "23                        1781               0.963016  \n",
      "11                        1123               0.828491  \n",
      "5                         1458               0.861536  \n",
      "14                         843               0.700331  \n",
      "6                         1391               0.922505  \n"
     ]
    }
   ],
   "source": [
    "df = df_checks.copy()\n",
    "\n",
    "# parse mins (strings look like dd/mm/yyyy)\n",
    "df[\"min_decision_date_dt\"] = pd.to_datetime(df[\"min_decision_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "train_counts = df[\"train_2019_2024_by_decision_date\"].fillna(0).astype(int)\n",
    "test_counts = df[\"test_2025_by_decision_date\"].fillna(0).astype(int)\n",
    "\n",
    "df[\"trainable_2019_2024_test_2025\"] = (train_counts >= MIN_TRAIN) & (test_counts >= MIN_TEST)\n",
    "\n",
    "total = len(df)\n",
    "trainable = int(df[\"trainable_2019_2024_test_2025\"].sum())\n",
    "not_trainable = total - trainable\n",
    "\n",
    "print(\"\\nTotal boroughs:\", total)\n",
    "print(f\"Trainable (decision_date) train 2019-2024 + test 2025: {trainable}\")\n",
    "print(\"NOT trainable:\", not_trainable)\n",
    "\n",
    "print(\"\\nNOT trainable list (lowest volumes first):\")\n",
    "print(\n",
    "    df.loc[~df[\"trainable_2019_2024_test_2025\"],\n",
    "           [\"borough\", \"min_decision_date\", \"train_2019_2024_by_decision_date\", \"test_2025_by_decision_date\",\n",
    "            \"pct_has_decision_date\"]]\n",
    "    .sort_values([\"train_2019_2024_by_decision_date\", \"test_2025_by_decision_date\"], ascending=[True, True])\n",
    ")\n",
    "\n",
    "print(\"\\nTrainable boroughs:\")\n",
    "print(\n",
    "    df.loc[df[\"trainable_2019_2024_test_2025\"],\n",
    "           [\"borough\", \"train_2019_2024_by_decision_date\", \"test_2025_by_decision_date\",\n",
    "            \"pct_has_decision_date\"]]\n",
    "    .sort_values([\"train_2019_2024_by_decision_date\"], ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65476f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct lpa_name.raw values containing 'Dagenham':\n",
      "- Barking & Dagenham: 14069\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "q = {\n",
    "    \"size\": 0,\n",
    "    \"track_total_hits\": True,\n",
    "    \"query\": {\n",
    "        \"wildcard\": {\"lpa_name.raw\": \"*Dagenham*\"}\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"lpa_names_raw\": {\n",
    "            \"terms\": {\"field\": \"lpa_name.raw\", \"size\": 50}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "\n",
    "if r.status_code != 200:\n",
    "    print(\"STATUS:\", r.status_code)\n",
    "    print(\"ERROR BODY:\\n\", r.text)  # this will tell you exactly what's wrong\n",
    "    r.raise_for_status()\n",
    "\n",
    "aggs = r.json()[\"aggregations\"][\"lpa_names_raw\"][\"buckets\"]\n",
    "\n",
    "print(\"Distinct lpa_name.raw values containing 'Dagenham':\")\n",
    "for b in aggs:\n",
    "    print(f\"- {b['key']}: {b['doc_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f39499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 565\n",
      "2020 920\n",
      "2021 948\n",
      "2022 748\n",
      "2023 773\n",
      "2024 641\n",
      "2025 265\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "borough = \"Barking & Dagenham\"\n",
    "\n",
    "q = {\n",
    "    \"size\": 0,\n",
    "    \"track_total_hits\": True,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"term\": {\"lpa_name.raw\": borough}},\n",
    "                {\"exists\": {\"field\": \"decision_date\"}},\n",
    "                {\n",
    "                    \"range\": {\n",
    "                        \"decision_date\": {\n",
    "                            \"gte\": \"01/01/2019\",\n",
    "                            \"lt\":  \"01/01/2026\",\n",
    "                            \"format\": \"dd/MM/yyyy\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"by_year\": {\n",
    "            \"date_histogram\": {\n",
    "                \"field\": \"decision_date\",\n",
    "                \"calendar_interval\": \"year\",\n",
    "                \"format\": \"yyyy\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "if r.status_code != 200:\n",
    "    print(\"STATUS:\", r.status_code)\n",
    "    print(r.text)\n",
    "    r.raise_for_status()\n",
    "\n",
    "buckets = r.json()[\"aggregations\"][\"by_year\"][\"buckets\"]\n",
    "for b in buckets:\n",
    "    print(b[\"key_as_string\"], b[\"doc_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "889a24e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_decision_date: 24/09/2025\n",
      "2025-01 55\n",
      "2025-02 28\n",
      "2025-03 57\n",
      "2025-04 50\n",
      "2025-05 74\n",
      "2025-06 0\n",
      "2025-07 0\n",
      "2025-08 0\n",
      "2025-09 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "borough = \"Barking & Dagenham\"\n",
    "\n",
    "q = {\n",
    "    \"size\": 0,\n",
    "    \"query\": {\"bool\": {\"must\": [\n",
    "        {\"term\": {\"lpa_name.raw\": borough}},\n",
    "        {\"exists\": {\"field\": \"decision_date\"}},\n",
    "        {\"range\": {\"decision_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\", \"format\": \"dd/MM/yyyy\"}}}\n",
    "    ]}},\n",
    "    \"aggs\": {\n",
    "        \"by_month\": {\n",
    "            \"date_histogram\": {\n",
    "                \"field\": \"decision_date\",\n",
    "                \"calendar_interval\": \"month\",\n",
    "                \"format\": \"yyyy-MM\"\n",
    "            }\n",
    "        },\n",
    "        \"max_decision_date\": {\"max\": {\"field\": \"decision_date\"}}\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "r.raise_for_status()\n",
    "\n",
    "aggs = r.json()[\"aggregations\"]\n",
    "print(\"max_decision_date:\", aggs[\"max_decision_date\"].get(\"value_as_string\"))\n",
    "\n",
    "for b in aggs[\"by_month\"][\"buckets\"]:\n",
    "    print(b[\"key_as_string\"], b[\"doc_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "781c5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_2025_by_valid_date: 1013\n",
      "has_decision_date_2025: 188\n"
     ]
    }
   ],
   "source": [
    "q2 = {\n",
    "    \"size\": 0,\n",
    "    \"query\": {\"bool\": {\"must\": [\n",
    "        {\"term\": {\"lpa_name.raw\": borough}},\n",
    "        {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\", \"format\": \"dd/MM/yyyy\"}}}\n",
    "    ]}},\n",
    "    \"aggs\": {\n",
    "        \"has_decision_date\": {\"filter\": {\"exists\": {\"field\": \"decision_date\"}}}\n",
    "    }\n",
    "}\n",
    "\n",
    "r2 = requests.post(API, headers=HDR, json=q2, timeout=60)\n",
    "r2.raise_for_status()\n",
    "j2 = r2.json()\n",
    "print(\"total_2025_by_valid_date:\", j2[\"hits\"][\"total\"][\"value\"])\n",
    "print(\"has_decision_date_2025:\", j2[\"aggregations\"][\"has_decision_date\"][\"doc_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1d98de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL (valid_date 2025): 1029\n",
      "HAS decision_date: 188\n",
      "MISSING decision_date: 841\n",
      "MISSING decision_date BUT decision field exists: 0\n",
      "\n",
      "Status breakdown where decision_date is missing:\n",
      "- Application Under Consideration: 841\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "borough = \"Barking & Dagenham\"\n",
    "\n",
    "q = {\n",
    "    \"size\": 0,\n",
    "    \"track_total_hits\": True,\n",
    "    \"query\": {\"bool\": {\"must\": [\n",
    "        {\"term\": {\"lpa_name.raw\": borough}},\n",
    "        {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\", \"format\": \"dd/MM/yyyy\"}}}\n",
    "    ]}},\n",
    "    \"aggs\": {\n",
    "        \"has_decision_date\": {\"filter\": {\"exists\": {\"field\": \"decision_date\"}}},\n",
    "        \"missing_decision_date\": {\n",
    "            \"filter\": {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "        },\n",
    "        \"missing_decision_date_by_status\": {\n",
    "            \"filter\": {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}},\n",
    "            \"aggs\": {\"status_terms\": {\"terms\": {\"field\": \"status.raw\", \"size\": 50}}}\n",
    "        },\n",
    "        \"missing_decision_date_has_decision_field\": {\n",
    "            \"filter\": {\"bool\": {\"must\": [\n",
    "                {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}},\n",
    "                {\"exists\": {\"field\": \"decision\"}}\n",
    "            ]}}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "r.raise_for_status()\n",
    "aggs = r.json()[\"aggregations\"]\n",
    "\n",
    "total = r.json()[\"hits\"][\"total\"][\"value\"]\n",
    "print(\"TOTAL (valid_date 2025):\", total)\n",
    "print(\"HAS decision_date:\", aggs[\"has_decision_date\"][\"doc_count\"])\n",
    "print(\"MISSING decision_date:\", aggs[\"missing_decision_date\"][\"doc_count\"])\n",
    "print(\"MISSING decision_date BUT decision field exists:\", aggs[\"missing_decision_date_has_decision_field\"][\"doc_count\"])\n",
    "\n",
    "print(\"\\nStatus breakdown where decision_date is missing:\")\n",
    "for b in aggs[\"missing_decision_date_by_status\"][\"status_terms\"][\"buckets\"]:\n",
    "    print(f\"- {b['key']}: {b['doc_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6b74af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 borough  total_valid_date_2025  has_decision_date_2025  \\\n",
      "0     Barking & Dagenham                   1029                     188   \n",
      "1         Waltham Forest                   1798                     517   \n",
      "2                Hackney                   1534                     599   \n",
      "3                 Harrow                   1401                     552   \n",
      "4   Hammersmith & Fulham                   3121                    1713   \n",
      "5                 Ealing                   2588                    1561   \n",
      "6         City of London                   1543                     969   \n",
      "7                 Camden                   1675                    1061   \n",
      "8               Hounslow                   2773                    1794   \n",
      "9              Islington                   3140                    2107   \n",
      "10               Bromley                   2065                    1403   \n",
      "11              Richmond                   3920                    2881   \n",
      "12         Tower Hamlets                   1916                    1466   \n",
      "13            Wandsworth                   4191                    3259   \n",
      "14             Greenwich                   3051                    2421   \n",
      "15              Lewisham                   1743                    1387   \n",
      "16           Westminster                   8489                    6765   \n",
      "17  Kensington & Chelsea                   4508                    3608   \n",
      "18             Southwark                   3505                    2818   \n",
      "19               Lambeth                   3611                    2910   \n",
      "20              Havering                   2886                    2331   \n",
      "21              Haringey                   2235                    1817   \n",
      "22                Newham                   2501                    2038   \n",
      "23                Bexley                   2412                    1976   \n",
      "24                Merton                   1827                    1501   \n",
      "25               Croydon                   3598                    2986   \n",
      "26            Hillingdon                   3198                    2703   \n",
      "27             Redbridge                   3827                    3252   \n",
      "28                Barnet                   5860                    4989   \n",
      "29              Kingston                   2710                    2329   \n",
      "30                 Brent                   3058                    2638   \n",
      "31                Sutton                   1857                    1606   \n",
      "32               Enfield                   3826                    3350   \n",
      "\n",
      "    missing_decision_date_2025  pct_decided_2025 top_missing_status  \\\n",
      "0                          841          0.182702                      \n",
      "1                         1281          0.287542                      \n",
      "2                          935          0.390482                      \n",
      "3                          849          0.394004                      \n",
      "4                         1408          0.548863                      \n",
      "5                         1027          0.603168                      \n",
      "6                          574          0.627997                      \n",
      "7                          614          0.633433                      \n",
      "8                          979          0.646953                      \n",
      "9                         1033          0.671019                      \n",
      "10                         662          0.679419                      \n",
      "11                        1039          0.734949                      \n",
      "12                         450          0.765136                      \n",
      "13                         932          0.777619                      \n",
      "14                         630          0.793510                      \n",
      "15                         356          0.795754                      \n",
      "16                        1724          0.796914                      \n",
      "17                         900          0.800355                      \n",
      "18                         687          0.803994                      \n",
      "19                         701          0.805871                      \n",
      "20                         555          0.807692                      \n",
      "21                         418          0.812975                      \n",
      "22                         463          0.814874                      \n",
      "23                         436          0.819237                      \n",
      "24                         326          0.821565                      \n",
      "25                         612          0.829906                      \n",
      "26                         495          0.845216                      \n",
      "27                         575          0.849752                      \n",
      "28                         871          0.851365                      \n",
      "29                         381          0.859410                      \n",
      "30                         420          0.862655                      \n",
      "31                         251          0.864836                      \n",
      "32                         476          0.875588                      \n",
      "\n",
      "    top_missing_status_pct_of_missing  eligible_2025_test error  \n",
      "0                                 0.0               False        \n",
      "1                                 0.0               False        \n",
      "2                                 0.0               False        \n",
      "3                                 0.0               False        \n",
      "4                                 0.0               False        \n",
      "5                                 0.0               False        \n",
      "6                                 0.0               False        \n",
      "7                                 0.0               False        \n",
      "8                                 0.0               False        \n",
      "9                                 0.0               False        \n",
      "10                                0.0               False        \n",
      "11                                0.0                True        \n",
      "12                                0.0                True        \n",
      "13                                0.0                True        \n",
      "14                                0.0                True        \n",
      "15                                0.0                True        \n",
      "16                                0.0                True        \n",
      "17                                0.0                True        \n",
      "18                                0.0                True        \n",
      "19                                0.0                True        \n",
      "20                                0.0                True        \n",
      "21                                0.0                True        \n",
      "22                                0.0                True        \n",
      "23                                0.0                True        \n",
      "24                                0.0                True        \n",
      "25                                0.0                True        \n",
      "26                                0.0                True        \n",
      "27                                0.0                True        \n",
      "28                                0.0                True        \n",
      "29                                0.0                True        \n",
      "30                                0.0                True        \n",
      "31                                0.0                True        \n",
      "32                                0.0                True        \n",
      "\n",
      "Saved: borough_2025_outcome_completeness.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "# Only needed if you ever see borough-name mismatches.\n",
    "# Your later test suggests lpa_name.raw is consistent, but this is harmless.\n",
    "BOROUGH_ALIASES = {\n",
    "    \"Barking & Dagenham\": [\"Barking & Dagenham\", \"Barking and Dagenham\"],\n",
    "    \"Hammersmith & Fulham\": [\"Hammersmith & Fulham\", \"Hammersmith and Fulham\"],\n",
    "    \"Kensington & Chelsea\": [\"Kensington & Chelsea\", \"Kensington and Chelsea\"],\n",
    "    \"Kingston\": [\"Kingston\", \"Kingston upon Thames\"],\n",
    "    \"Richmond\": [\"Richmond\", \"Richmond upon Thames\"],\n",
    "}\n",
    "\n",
    "def borough_terms(borough: str):\n",
    "    return BOROUGH_ALIASES.get(borough, [borough])\n",
    "\n",
    "def post_json(session: requests.Session, payload: dict, timeout=60):\n",
    "    r = session.post(API, headers=HDR, json=payload, timeout=timeout)\n",
    "    if r.status_code >= 400:\n",
    "        # Print full error text so you can immediately see parse issues (e.g., wrong date format)\n",
    "        raise requests.HTTPError(f\"HTTP {r.status_code}: {r.text}\", response=r)\n",
    "    return r.json()\n",
    "\n",
    "def borough_2025_outcome_completeness(session: requests.Session, borough: str):\n",
    "    names = borough_terms(borough)\n",
    "\n",
    "    # IMPORTANT: this API expects dd/MM/yyyy (you already got burned by yyyy-mm-dd)\n",
    "    start_2025 = \"01/01/2025\"\n",
    "    start_2026 = \"01/01/2026\"\n",
    "\n",
    "    # Base filter: applications with valid_date in 2025, for this borough\n",
    "    base_filter = {\n",
    "        \"bool\": {\"must\": [\n",
    "            {\"terms\": {\"lpa_name.raw\": names}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": start_2025, \"lt\": start_2026}}}\n",
    "        ]}\n",
    "    }\n",
    "\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": base_filter,\n",
    "        \"aggs\": {\n",
    "            # total 2025 by valid_date is hits.total\n",
    "            \"has_decision_date\": {\"filter\": {\"exists\": {\"field\": \"decision_date\"}}},\n",
    "\n",
    "            \"missing_decision_date\": {\n",
    "                \"filter\": {\"bool\": {\"must\": [\n",
    "                    {\"terms\": {\"lpa_name.raw\": names}},\n",
    "                    {\"range\": {\"valid_date\": {\"gte\": start_2025, \"lt\": start_2026}}},\n",
    "                    {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "                ]}}\n",
    "            },\n",
    "\n",
    "            # what statuses dominate among missing decision_date?\n",
    "            \"status_breakdown_missing_decision_date\": {\n",
    "                \"filter\": {\"bool\": {\"must\": [\n",
    "                    {\"terms\": {\"lpa_name.raw\": names}},\n",
    "                    {\"range\": {\"valid_date\": {\"gte\": start_2025, \"lt\": start_2026}}},\n",
    "                    {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "                ]}},\n",
    "                \"aggs\": {\n",
    "                    \"by_status\": {\"terms\": {\"field\": \"status.keyword\", \"size\": 20}}\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    j = post_json(session, q)\n",
    "\n",
    "    total_2025 = j[\"hits\"][\"total\"][\"value\"]\n",
    "    decided_2025 = j[\"aggregations\"][\"has_decision_date\"][\"doc_count\"]\n",
    "    missing_2025 = total_2025 - decided_2025\n",
    "    pct_decided = (decided_2025 / total_2025) if total_2025 else 0.0\n",
    "\n",
    "    # pull top status for missing decision_date\n",
    "    buckets = j[\"aggregations\"][\"status_breakdown_missing_decision_date\"][\"by_status\"][\"buckets\"]\n",
    "    top_status = buckets[0][\"key\"] if buckets else \"\"\n",
    "    top_status_count = buckets[0][\"doc_count\"] if buckets else 0\n",
    "    top_status_pct_missing = (top_status_count / missing_2025) if missing_2025 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"borough\": borough,\n",
    "        \"names_used\": \"|\".join(names),\n",
    "        \"total_valid_date_2025\": int(total_2025),\n",
    "        \"has_decision_date_2025\": int(decided_2025),\n",
    "        \"missing_decision_date_2025\": int(missing_2025),\n",
    "        \"pct_decided_2025\": float(pct_decided),\n",
    "        \"top_missing_status\": top_status,\n",
    "        \"top_missing_status_count\": int(top_status_count),\n",
    "        \"top_missing_status_pct_of_missing\": float(top_status_pct_missing),\n",
    "    }\n",
    "\n",
    "def run_boroughwide_2025_check(min_pct_decided=0.70, min_decided_abs=200):\n",
    "    \"\"\"\n",
    "    Eligibility rule (edit if needed):\n",
    "      - pct_decided_2025 >= min_pct_decided\n",
    "      - AND has_decision_date_2025 >= min_decided_abs (avoid tiny denominators)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with requests.Session() as session:\n",
    "        for b in BOROUGHS:\n",
    "            try:\n",
    "                rows.append(borough_2025_outcome_completeness(session, b))\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"borough\": b,\n",
    "                    \"names_used\": \"|\".join(borough_terms(b)),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # If there were errors, keep them visible\n",
    "    if \"error\" not in df.columns:\n",
    "        df[\"error\"] = \"\"\n",
    "\n",
    "    # Eligibility flags\n",
    "    df[\"eligible_2025_test\"] = (\n",
    "        (df[\"pct_decided_2025\"].fillna(0) >= min_pct_decided) &\n",
    "        (df[\"has_decision_date_2025\"].fillna(0).astype(float) >= float(min_decided_abs)) &\n",
    "        (df[\"error\"].fillna(\"\") == \"\")\n",
    "    )\n",
    "\n",
    "    # Sort worst-first for quick diagnosis\n",
    "    df = df.sort_values(\n",
    "        by=[\"eligible_2025_test\", \"pct_decided_2025\", \"has_decision_date_2025\", \"total_valid_date_2025\"],\n",
    "        ascending=[True, True, True, True],\n",
    "        na_position=\"last\"\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- RUN IT\n",
    "df_2025 = run_boroughwide_2025_check(min_pct_decided=0.70, min_decided_abs=200)\n",
    "\n",
    "print(df_2025[[\n",
    "    \"borough\", \"total_valid_date_2025\", \"has_decision_date_2025\", \"missing_decision_date_2025\",\n",
    "    \"pct_decided_2025\", \"top_missing_status\", \"top_missing_status_pct_of_missing\", \"eligible_2025_test\", \"error\"\n",
    "]])\n",
    "\n",
    "# Save to CSV so you can share or attach later\n",
    "out_path = \"borough_2025_outcome_completeness.csv\"\n",
    "df_2025.to_csv(out_path, index=False)\n",
    "print(\"\\nSaved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac64cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: status.raw | HTTP: 200\n",
      "Buckets: [('Application Under Consideration', 841)]\n",
      "Field: status.keyword | HTTP: 200\n",
      "Buckets: []\n",
      "Field: status | HTTP: 400\n",
      "{\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [status] in order to load field data by uninverting the inverted index. Note that this can use significant memory.\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"applications\",\"node\":\"YjOvxhioRamyfuNMwJ22Jg\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [status] in order to load field data by uninverting the inverted index. Note that this can use significant memory.\"}}],\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [status] in order to load field data by uninverting the inverted index. Note that this can use significant memory.\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [status] in order to load field data by uninverting the inverted index. Note that this can use significant memory.\"}}},\"status\":400}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def check_status_field(borough, status_field):\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"term\": {\"lpa_name.raw\": borough}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "            {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "        ]}},\n",
    "        \"aggs\": {\"status_breakdown\": {\"terms\": {\"field\": status_field, \"size\": 20}}}\n",
    "    }\n",
    "    r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "    print(\"Field:\", status_field, \"| HTTP:\", r.status_code)\n",
    "    if r.status_code != 200:\n",
    "        print(r.text)\n",
    "        return\n",
    "    buckets = r.json()[\"aggregations\"][\"status_breakdown\"][\"buckets\"]\n",
    "    print(\"Buckets:\", [(b[\"key\"], b[\"doc_count\"]) for b in buckets[:10]])\n",
    "\n",
    "for f in [\"status.raw\", \"status.keyword\", \"status\"]:\n",
    "    check_status_field(\"Barking & Dagenham\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b91785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: borough_2025_outcome_completeness_FIXED.csv\n",
      "                 borough  total_valid_date_2025  has_decision_date_2025  \\\n",
      "0     Barking & Dagenham                   1029                     188   \n",
      "30        Waltham Forest                   1798                     517   \n",
      "11               Hackney                   1534                     599   \n",
      "14                Harrow                   1401                     552   \n",
      "12  Hammersmith & Fulham                   3133                    1717   \n",
      "8                 Ealing                   2588                    1561   \n",
      "6         City of London                   1545                     972   \n",
      "5                 Camden                   1675                    1061   \n",
      "17              Hounslow                   2775                    1808   \n",
      "18             Islington                   3144                    2126   \n",
      "4                Bromley                   2065                    1403   \n",
      "26              Richmond                   3920                    2893   \n",
      "29         Tower Hamlets                   1929                    1470   \n",
      "31            Wandsworth                   4194                    3310   \n",
      "10             Greenwich                   3052                    2428   \n",
      "\n",
      "    missing_decision_date_2025  pct_decided_2025  \\\n",
      "0                          841          0.182702   \n",
      "30                        1281          0.287542   \n",
      "11                         935          0.390482   \n",
      "14                         849          0.394004   \n",
      "12                        1416          0.548037   \n",
      "8                         1027          0.603168   \n",
      "6                          573          0.629126   \n",
      "5                          614          0.633433   \n",
      "17                         967          0.651532   \n",
      "18                        1018          0.676209   \n",
      "4                          662          0.679419   \n",
      "26                        1027          0.738010   \n",
      "29                         459          0.762053   \n",
      "31                         884          0.789223   \n",
      "10                         624          0.795544   \n",
      "\n",
      "                 top_missing_status  top_missing_status_pct_of_missing error  \\\n",
      "0   Application Under Consideration                           1.000000         \n",
      "30  Application Under Consideration                           0.999219         \n",
      "11  Application Under Consideration                           0.993583         \n",
      "14  Application Under Consideration                           1.000000         \n",
      "12  Application Under Consideration                           0.999294         \n",
      "8                __MISSING_STATUS__                           0.938656         \n",
      "6   Application Under Consideration                           1.000000         \n",
      "5              Application Received                           0.998371         \n",
      "17             Application Received                           1.000000         \n",
      "18  Application Under Consideration                           0.775049         \n",
      "4   Application Under Consideration                           1.000000         \n",
      "26  Application Under Consideration                           0.949367         \n",
      "29  Application Under Consideration                           0.989107         \n",
      "31             Application Received                           0.994344         \n",
      "10  Application Under Consideration                           0.947115         \n",
      "\n",
      "    eligible_2025_test  \n",
      "0                False  \n",
      "30               False  \n",
      "11               False  \n",
      "14               False  \n",
      "12               False  \n",
      "8                False  \n",
      "6                False  \n",
      "5                False  \n",
      "17               False  \n",
      "18               False  \n",
      "4                False  \n",
      "26                True  \n",
      "29                True  \n",
      "31                True  \n",
      "10                True  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "BOROUGHS = [\n",
    "    \"Barking & Dagenham\", \"Barnet\", \"Bexley\", \"Brent\", \"Bromley\",\n",
    "    \"Camden\", \"City of London\", \"Croydon\", \"Ealing\", \"Enfield\",\n",
    "    \"Greenwich\", \"Hackney\", \"Hammersmith & Fulham\", \"Haringey\",\n",
    "    \"Harrow\", \"Havering\", \"Hillingdon\", \"Hounslow\", \"Islington\",\n",
    "    \"Kensington & Chelsea\", \"Kingston\", \"Lambeth\", \"Lewisham\",\n",
    "    \"Merton\", \"Newham\", \"Redbridge\", \"Richmond\", \"Southwark\",\n",
    "    \"Sutton\", \"Tower Hamlets\", \"Waltham Forest\", \"Wandsworth\",\n",
    "    \"Westminster\"\n",
    "]\n",
    "\n",
    "def borough_2025_completeness(session: requests.Session, borough: str) -> dict:\n",
    "    \"\"\"\n",
    "    Cohort = valid_date in 2025.\n",
    "    Measures: how many already have decision_date + most common missing status (status.raw).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Total in 2025 by valid_date\n",
    "        q_total = {\n",
    "            \"size\": 0,\n",
    "            \"track_total_hits\": True,\n",
    "            \"query\": {\"bool\": {\"must\": [\n",
    "                {\"term\": {\"lpa_name.raw\": borough}},\n",
    "                {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}}\n",
    "            ]}}\n",
    "        }\n",
    "\n",
    "        # Those with decision_date present\n",
    "        q_has_dec = {\n",
    "            \"size\": 0,\n",
    "            \"track_total_hits\": True,\n",
    "            \"query\": {\"bool\": {\"must\": [\n",
    "                {\"term\": {\"lpa_name.raw\": borough}},\n",
    "                {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "                {\"exists\": {\"field\": \"decision_date\"}}\n",
    "            ]}}\n",
    "        }\n",
    "\n",
    "        # Missing decision_date -> status breakdown (IMPORTANT: status.raw)\n",
    "        q_missing_status = {\n",
    "            \"size\": 0,\n",
    "            \"track_total_hits\": True,\n",
    "            \"query\": {\"bool\": {\"must\": [\n",
    "                {\"term\": {\"lpa_name.raw\": borough}},\n",
    "                {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "                {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "            ]}},\n",
    "            \"aggs\": {\n",
    "                \"status_breakdown\": {\"terms\": {\"field\": \"status.raw\", \"size\": 20, \"missing\": \"__MISSING_STATUS__\"}}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        r1 = session.post(API, headers=HDR, json=q_total, timeout=60); r1.raise_for_status()\n",
    "        r2 = session.post(API, headers=HDR, json=q_has_dec, timeout=60); r2.raise_for_status()\n",
    "        r3 = session.post(API, headers=HDR, json=q_missing_status, timeout=60); r3.raise_for_status()\n",
    "\n",
    "        total = int(r1.json()[\"hits\"][\"total\"][\"value\"])\n",
    "        has_dec = int(r2.json()[\"hits\"][\"total\"][\"value\"])\n",
    "        missing = total - has_dec\n",
    "        pct_decided = (has_dec / total) if total else 0.0\n",
    "\n",
    "        buckets = r3.json()[\"aggregations\"][\"status_breakdown\"][\"buckets\"]\n",
    "        if missing > 0 and len(buckets) > 0:\n",
    "            top_status = buckets[0][\"key\"]\n",
    "            top_status_pct_of_missing = buckets[0][\"doc_count\"] / missing\n",
    "        else:\n",
    "            top_status = \"\"\n",
    "            top_status_pct_of_missing = 0.0\n",
    "\n",
    "        return {\n",
    "            \"borough\": borough,\n",
    "            \"total_valid_date_2025\": total,\n",
    "            \"has_decision_date_2025\": has_dec,\n",
    "            \"missing_decision_date_2025\": missing,\n",
    "            \"pct_decided_2025\": pct_decided,\n",
    "            \"top_missing_status\": top_status,\n",
    "            \"top_missing_status_pct_of_missing\": top_status_pct_of_missing,\n",
    "            \"error\": \"\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"borough\": borough,\n",
    "            \"total_valid_date_2025\": None,\n",
    "            \"has_decision_date_2025\": None,\n",
    "            \"missing_decision_date_2025\": None,\n",
    "            \"pct_decided_2025\": None,\n",
    "            \"top_missing_status\": \"\",\n",
    "            \"top_missing_status_pct_of_missing\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def run_borough_table_2025(out_csv_path=\"borough_2025_outcome_completeness_FIXED.csv\"):\n",
    "    rows = []\n",
    "    with requests.Session() as session:\n",
    "        for b in BOROUGHS:\n",
    "            rows.append(borough_2025_completeness(session, b))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Example eligibility rule (edit as you like)\n",
    "    # Here: at least 70% already decided in the 2025 valid_date cohort\n",
    "    df[\"eligible_2025_test\"] = df[\"pct_decided_2025\"].fillna(0) >= 0.70\n",
    "\n",
    "    df = df.sort_values([\"pct_decided_2025\", \"total_valid_date_2025\"], ascending=[True, False])\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "\n",
    "    print(\"Saved:\", out_csv_path)\n",
    "    return df\n",
    "\n",
    "df_fixed = run_borough_table_2025()\n",
    "print(df_fixed.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc245f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            status  count\n",
      "0  Application Under Consideration     63\n",
      "Total missing (sum buckets): 63\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def missing_status_breakdown(borough: str, size=20):\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"term\": {\"lpa_name.raw\": borough}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "            {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "        ]}},\n",
    "        \"aggs\": {\n",
    "            \"status_breakdown\": {\"terms\": {\"field\": \"status.raw\", \"size\": size}}\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    buckets = r.json()[\"aggregations\"][\"status_breakdown\"][\"buckets\"]\n",
    "    return pd.DataFrame(buckets).rename(columns={\"key\": \"status\", \"doc_count\": \"count\"})\n",
    "\n",
    "df_ealing = missing_status_breakdown(\"Ealing\", size=30)\n",
    "print(df_ealing.head(30))\n",
    "print(\"Total missing (sum buckets):\", df_ealing[\"count\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af387fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing decision_date: 1027\n",
      "…with status.raw: 63\n",
      "…without status.raw: 964\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API = \"https://planningdata.london.gov.uk/api-guest/applications/_search\"\n",
    "HDR = {\"X-API-AllowRequest\": \"be2rmRnt&\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def missing_decision_status_exists(borough: str):\n",
    "    base_must = [\n",
    "        {\"term\": {\"lpa_name.raw\": borough}},\n",
    "        {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "        {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "    ]\n",
    "\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\"bool\": {\"must\": base_must}},\n",
    "        \"aggs\": {\n",
    "            \"status_exists\": {\"filter\": {\"exists\": {\"field\": \"status.raw\"}}},\n",
    "            \"status_missing\": {\"filter\": {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"status.raw\"}}]}}},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    aggs = r.json()[\"aggregations\"]\n",
    "\n",
    "    total_missing_decision = r.json()[\"hits\"][\"total\"][\"value\"]\n",
    "    with_status = aggs[\"status_exists\"][\"doc_count\"]\n",
    "    without_status = aggs[\"status_missing\"][\"doc_count\"]\n",
    "\n",
    "    return total_missing_decision, with_status, without_status\n",
    "\n",
    "total, with_status, without_status = missing_decision_status_exists(\"Ealing\")\n",
    "print(\"Total missing decision_date:\", total)\n",
    "print(\"…with status.raw:\", with_status)\n",
    "print(\"…without status.raw:\", without_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd37354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            status  count\n",
      "0               __MISSING_STATUS__    964\n",
      "1  Application Under Consideration     63\n",
      "Total missing decision_date (sum buckets): 1027\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def missing_status_breakdown_with_missing_bucket(borough: str, size=20):\n",
    "    q = {\n",
    "        \"size\": 0,\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"term\": {\"lpa_name.raw\": borough}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "            {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}}\n",
    "        ]}},\n",
    "        \"aggs\": {\n",
    "            \"status_breakdown\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": \"status.raw\",\n",
    "                    \"size\": size,\n",
    "                    \"missing\": \"__MISSING_STATUS__\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    buckets = r.json()[\"aggregations\"][\"status_breakdown\"][\"buckets\"]\n",
    "    df = pd.DataFrame(buckets).rename(columns={\"key\": \"status\", \"doc_count\": \"count\"})\n",
    "    return df\n",
    "\n",
    "df = missing_status_breakdown_with_missing_bucket(\"Ealing\", size=30)\n",
    "print(df.head(30))\n",
    "print(\"Total missing decision_date (sum buckets):\", df[\"count\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed032e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  lpa_name decision_date  valid_date decision                id status\n",
      "0   Ealing          None  03/06/2025     None  Ealing-252129CND   None\n",
      "1   Ealing          None  15/05/2025     None  Ealing-251902ADV   None\n",
      "2   Ealing          None  09/06/2025     None  Ealing-252215CND   None\n",
      "3   Ealing          None  23/05/2025     None  Ealing-252003CND   None\n",
      "4   Ealing          None  30/04/2025     None  Ealing-251703ADV   None\n",
      "5   Ealing          None  04/02/2025     None  Ealing-250457FUL   None\n",
      "6   Ealing          None  03/06/2025     None  Ealing-252321CND   None\n",
      "7   Ealing          None  27/02/2025     None  Ealing-250715CPE   None\n",
      "8   Ealing          None  02/06/2025     None  Ealing-251847FUL   None\n",
      "9   Ealing          None  15/05/2025     None  Ealing-251513FUL   None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def sample_missing_status_docs(borough: str, n=10):\n",
    "    q = {\n",
    "        \"size\": n,\n",
    "        \"_source\": [\"id\", \"lpa_name\", \"valid_date\", \"decision_date\", \"status\", \"decision\"],\n",
    "        \"query\": {\"bool\": {\"must\": [\n",
    "            {\"term\": {\"lpa_name.raw\": borough}},\n",
    "            {\"range\": {\"valid_date\": {\"gte\": \"01/01/2025\", \"lt\": \"01/01/2026\"}}},\n",
    "            {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"decision_date\"}}]}},\n",
    "            {\"bool\": {\"must_not\": [{\"exists\": {\"field\": \"status.raw\"}}]}}\n",
    "        ]}}\n",
    "    }\n",
    "    r = requests.post(API, headers=HDR, json=q, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    hits = r.json()[\"hits\"][\"hits\"]\n",
    "    return pd.json_normalize([h[\"_source\"] for h in hits])\n",
    "\n",
    "print(sample_missing_status_docs(\"Ealing\", n=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
